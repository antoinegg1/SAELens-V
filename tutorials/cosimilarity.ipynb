{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/changye/miniconda3/envs/sae/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import google.colab # type: ignore\n",
    "    from google.colab import output\n",
    "    COLAB = True\n",
    "    %pip install sae-lens transformer-lens\n",
    "except:\n",
    "    COLAB = False\n",
    "    from IPython import get_ipython # type: ignore\n",
    "    ipython = get_ipython(); assert ipython is not None\n",
    "    ipython.run_line_magic(\"load_ext\", \"autoreload\")\n",
    "    ipython.run_line_magic(\"autoreload\", \"2\")\n",
    "\n",
    "# Standard imports\n",
    "import os\n",
    "import torch\n",
    "import tqdm\n",
    "import plotly.express as px  \n",
    "import random\n",
    "from datasets import Dataset, DatasetDict, IterableDataset, load_dataset,load_from_disk\n",
    "from transformer_lens import HookedTransformer\n",
    "from typing import Any, Generator, Iterator, Literal, cast\n",
    "from sae_lens import SAE\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    LlavaNextForConditionalGeneration,\n",
    "    LlavaNextProcessor,\n",
    "    AutoModelForCausalLM,\n",
    ")\n",
    "import numpy as np\n",
    "os.environ[\"http_proxy\"] = \"http://127.0.0.1:7895\"\n",
    "os.environ[\"https_proxy\"] = \"http://127.0.0.1:7895\"\n",
    "import concurrent.futures\n",
    "from transformer_lens.HookedLlava import HookedLlava\n",
    "from sae_lens.activation_visualization import (\n",
    "    load_llava_model,\n",
    "    load_sae,\n",
    "    separate_feature,\n",
    "    run_model,\n",
    "    cal_top_cosimilarity,\n",
    ")\n",
    "\n",
    "model_name = \"llava-hf/llava-v1.6-mistral-7b-hf\"\n",
    "model_path=\"/mnt/file2/changye/model/llava_AA_preference_cocour_0_25\"\n",
    "sae_path=\"/mnt/file2/changye/model/llavasae_obliec100k_SAEV\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantization check time: 0.00s\n",
      "Configuration loading time: 0.00s\n",
      "Model configuration processing time: 0.81s\n",
      "State dict loading time: 0.01s\n",
      "Tokenizer setup time: 0.38s\n",
      "Embedding setup time: 70.43s\n",
      "Move device time: 0.00s\n",
      "Set up time: 0.00s\n",
      "Model creation time: 70.81s\n",
      "State dict processing time: 44.40s\n",
      "Device moving time: 33.11s\n",
      "Total loading time: 149.15s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/file2/changye/SAELens-V/sae_lens/sae.py:136: UserWarning: \n",
      "This SAE has non-empty model_from_pretrained_kwargs. \n",
      "For optimal performance, load the model like so:\n",
      "model = HookedSAETransformer.from_pretrained_no_processing(..., **cfg.model_from_pretrained_kwargs)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6,7\" \n",
    "sae_device=\"cuda:7\"\n",
    "device=\"cuda:0\"\n",
    "processor,  hook_language_model = load_llava_model(\n",
    "        model_name, model_path, device,n_devices=8,stop_at_layer=17\n",
    "    )\n",
    "sae = load_sae(sae_path, sae_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question', 'image', 'response_1', 'response_2', 'res_1_from', 'res_2_from', 'p_response', 'prompt_following_rate_1', 'prompt_following_rate_2', 'p_rationale_1', 'p_rationale_2', 'o_response', 'objective_rules_rate_1', 'objective_rules_rate_2', 'o_rationale_1', 'o_rationale_2', 'c_response', 'clarity_rate_1', 'clarity_rate_2', 'c_rationale_1', 'c_rationale_2', 'i_response', 'information_richness_rate_1', 'information_richness_rate_2', 'i_rationale_1', 'i_rationale_2', 's_response', 'safety_rate_1', 'safety_rate_2', 's_rationale_1', 's_rationale_2', 'text_critique_1', 'text_critique_2', 'overall_response', 'overall_textfeedback'],\n",
      "    num_rows: 1000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "#1.通过采样集给sae feature 评分\n",
    "## 1.1 读取采样集\n",
    "dataset_path=\"/mnt/file2/changye/dataset/Align-Anything_preference_1k\"\n",
    "system_prompt= \" \"\n",
    "user_prompt= 'USER: \\n<image> {input}'\n",
    "assistant_prompt= '\\nASSISTANT: {output}'\n",
    "split_token= 'ASSISTANT:' \n",
    "eval_dataset = load_from_disk(\n",
    "            dataset_path,\n",
    "        )\n",
    "\n",
    "total_size = len(eval_dataset)\n",
    "\n",
    "print(eval_dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1.2 组合input\n",
    "\n",
    "# def prepare_inputs(i,formatted_sample,processor):\n",
    "#     prompt=formatted_sample['question'][i]\n",
    "#     formatted_prompt = f'{system_prompt}{user_prompt.format(input=prompt)}{assistant_prompt.format(output=\"\")}'\n",
    "#     image = formatted_sample['image'][i]\n",
    "#     image = image.resize((336, 336)).convert('RGBA')\n",
    "#     \"\"\"处理样本并准备输入\"\"\"\n",
    "#     text_input = processor.tokenizer(formatted_prompt, return_tensors=\"pt\")\n",
    "#     image_input = processor.image_processor(images=image, return_tensors=\"pt\")\n",
    "#     \"\"\"处理样本并准备输入\"\"\"\n",
    "#     return {\n",
    "#         \"input_ids\": text_input[\"input_ids\"],\n",
    "#         \"attention_mask\": text_input[\"attention_mask\"],\n",
    "#         \"pixel_values\": image_input[\"pixel_values\"],\n",
    "#         \"image_sizes\": image_input[\"image_sizes\"],\n",
    "#     }\n",
    "\n",
    "# inputs=[]\n",
    "# with tqdm.tqdm(total=total_size, desc=\"Processing batches\") as pbar:\n",
    "#         max_threads = 60  # 设置你希望的线程数，可以根据需要调整\n",
    "#         # print(\"os cpu counts\",os.cpu_count())\n",
    "#         with concurrent.futures.ThreadPoolExecutor(max_workers=max_threads) as executor:\n",
    "#             futures = []\n",
    "#             # 提交所有任务到线程池\n",
    "#             for i in range(0, total_size):\n",
    "#                 future = executor.submit(prepare_inputs, \n",
    "#                                         i, \n",
    "#                                         eval_dataset, \n",
    "#                                         processor, \n",
    "#                                        )\n",
    "#                 futures.append(future)\n",
    "\n",
    "#             # 使用 as_completed 确保任务完成后及时更新进度条\n",
    "#             for future in concurrent.futures.as_completed(futures):\n",
    "#                 input = future.result()  # 获取每个批次处理后的结果\n",
    "#                 inputs.append(input)  # 将处理的结果添加到 all_inputs\n",
    "#                 pbar.update(1)  # 更新进度条\n",
    "#                 pbar.refresh()  # 强制刷新进度条，确保显示更新\n",
    "    \n",
    "# print(inputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_example(example, \n",
    "                          system_prompt, \n",
    "                          user_prompt, \n",
    "                          assistant_prompt, \n",
    "                          processor):\n",
    "    \"\"\"\n",
    "    example: Dataset 中的一条样本, 包含键: {'question': ..., 'image': ...}\n",
    "    \"\"\"\n",
    "    # 构造文本提示\n",
    "    prompt = example['question']\n",
    "    formatted_prompt = (\n",
    "        f\"{system_prompt}\"\n",
    "        f\"{user_prompt.format(input=prompt)}\"\n",
    "        f\"{assistant_prompt.format(output='')}\"\n",
    "    )\n",
    "\n",
    "    # 处理图像 (PIL)\n",
    "    image = example['image']\n",
    "    image = image.resize((336, 336)).convert('RGBA')\n",
    "    \n",
    "    # 处理文本\n",
    "    text_input = processor.tokenizer(formatted_prompt, return_tensors=\"pt\")\n",
    "    # 处理图像\n",
    "    image_input = processor.image_processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "    # 注意：HuggingFace Dataset 要求返回的每个 key 对应“可序列化”的 numpy/pytorch/pyarrow 对象\n",
    "    # 如果返回的是 tensors，需要转成 list 或 numpy；也可直接返回 tensor（从 HF Datasets 2.6 开始支持）\n",
    "    # 这里示例直接返回张量的 shape=[1, ...]，后续你在使用时再做处理\n",
    "    return {\n",
    "        \"input_ids\": text_input[\"input_ids\"],\n",
    "        \"attention_mask\": text_input[\"attention_mask\"],\n",
    "        \"pixel_values\": image_input[\"pixel_values\"],\n",
    "        \"image_sizes\": image_input[\"image_sizes\"]\n",
    "    }\n",
    "\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "# 将上述处理函数+固定的参数打包（system_prompt, user_prompt等）\n",
    "process_fn = partial(\n",
    "    process_single_example,\n",
    "    system_prompt=system_prompt,\n",
    "    user_prompt=user_prompt,\n",
    "    assistant_prompt=assistant_prompt,\n",
    "    processor=processor\n",
    ")\n",
    "\n",
    "# 使用 map 并行处理\n",
    "# num_proc 指定 CPU 进程数。根据机器配置和内存大小调整，太大可能内存不足或进程调度开销变大。\n",
    "inputs = eval_dataset.map(\n",
    "    process_fn,\n",
    "    num_proc=60,          # 这里示例用 8，实际可根据 CPU 核数或内存情况调整\n",
    "    batched=False,       # 针对每条记录调用 process_fn\n",
    "    desc=\"Processing dataset\",\n",
    "    remove_columns=eval_dataset.column_names,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[336, 336]]\n"
     ]
    }
   ],
   "source": [
    "print(inputs[0][\"image_sizes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing cosimilarity: 100%|██████████| 1000/1000 [51:01<00:00,  3.06s/it] \n"
     ]
    }
   ],
   "source": [
    "text_token_meta_list=[]\n",
    "image_token_meta_list=[]\n",
    "with tqdm.tqdm(total=len(inputs), desc=\"Processing cosimilarity\") as pbar:\n",
    "    for input in inputs:\n",
    "        input_ids = torch.tensor(input[\"input_ids\"]).clone().to(device)\n",
    "        attention_mask =  torch.tensor(input[\"attention_mask\"]).clone().to(device)\n",
    "        pixel_values =  torch.tensor(input[\"pixel_values\"]).clone().to(device)\n",
    "        image_sizes =  torch.tensor(input[\"image_sizes\"]).clone().to(device)\n",
    "\n",
    "        # 构建包含移动后张量的新的输入字典\n",
    "        processed_input = {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"pixel_values\": pixel_values,\n",
    "            \"image_sizes\": image_sizes,\n",
    "        }\n",
    "        tmp_cache,image_indices, feature_act = run_model(processed_input, hook_language_model, sae, sae_device,stop_at_layer=17)\n",
    "        # print(image_indices.shape)\n",
    "        text_token_list, image_token_list=cal_top_cosimilarity(tmp_cache[0],image_indices[0], feature_act[0])\n",
    "        text_token_meta_list.append(text_token_list)\n",
    "        image_token_meta_list.append(image_token_list)\n",
    "        pbar.update(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def save_sliced_pt(data_list, output_folder, file_prefix, slice_size):\n",
    "#     \"\"\"\n",
    "#     将大列表切分成多个小文件保存为 .pt 格式。\n",
    "    \n",
    "#     参数:\n",
    "#     - data_list: 待保存的列表。\n",
    "#     - output_folder: 输出文件夹路径。\n",
    "#     - file_prefix: 保存文件的前缀名。\n",
    "#     - slice_size: 每个小文件的最大元素数。\n",
    "#     \"\"\"\n",
    "#     os.makedirs(output_folder, exist_ok=True)  # 确保文件夹存在\n",
    "    \n",
    "#     num_slices = (len(data_list) + slice_size - 1) // slice_size  # 计算分片数量\n",
    "#     for i in range(num_slices):\n",
    "#         start_idx = i * slice_size\n",
    "#         end_idx = min((i + 1) * slice_size, len(data_list))\n",
    "#         slice_data = data_list[start_idx:end_idx]\n",
    "        \n",
    "#         # 构造文件名\n",
    "#         file_path = os.path.join(output_folder, f\"{file_prefix}_part_{i + 1}.pt\")\n",
    "#         torch.save(slice_data, file_path)\n",
    "#         print(f\"Saved slice {i + 1} to {file_path}\")\n",
    "\n",
    "# # 假设你已经准备好了 `flattened_text_list` 和 `flattened_image_list`\n",
    "flattened_text_list = np.concatenate(text_token_meta_list).tolist()\n",
    "flattened_image_list = np.concatenate(image_token_meta_list).tolist()\n",
    "\n",
    "# # 设置保存参数\n",
    "# output_folder = \"/data/changye/output_folder_path\"  # 替换为实际路径\n",
    "# text_prefix = \"flattened_text_list\"\n",
    "# image_prefix = \"flattened_image_list\"\n",
    "# slice_size = 50000  # 每个文件保存 10000 条数据\n",
    "\n",
    "# # 保存分片的文件\n",
    "# save_sliced_pt(flattened_text_list, output_folder, text_prefix, slice_size)\n",
    "# save_sliced_pt(flattened_image_list, output_folder, image_prefix, slice_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing flattened_text_list: 100%|██████████| 29763/29763 [03:10<00:00, 156.58it/s] \n",
      "Processing flattened_image_list: 100%|██████████| 1176000/1176000 [30:44<00:00, 637.67it/s] \n",
      "Processing feature_num: 100%|██████████| 65536/65536 [01:53<00:00, 576.73it/s]\n"
     ]
    }
   ],
   "source": [
    "#我现在有两个list,flattened_text_list和flattend_image_list,他们的每个元素,称之为token都包含两个字典dict_keys(['features', 'logits']),其中features这个key对应的value也是一个字典,它的key为feature_index,value为feature_activation_value,\n",
    "#现在我希望对每个feature对照一个排序,每个feature要排序出它激活值最高的30个token(假设features中的子字典没有这个feature的index,则说明它在该token的激活值为0),并且能够区分哪些token是text的,哪些是image的.请补全以下代码:\n",
    "feature_num = 65536  # 定义特征的总数量\n",
    "features_top = [[] for _ in range(feature_num)]  # 初始化每个特征的列表，用于存储对应的 tokens\n",
    "\n",
    "# 处理文本 tokens\n",
    "with tqdm.tqdm(total=len(flattened_text_list), desc=\"Processing flattened_text_list\") as pbar:\n",
    "    for token in flattened_text_list:\n",
    "        for feature_index, activation_value in token['features'].items():\n",
    "            # 将激活值、token 和类型 ('text') 添加到对应特征的列表中\n",
    "            features_top[feature_index].append((activation_value, token, 'text'))\n",
    "        pbar.update(1)\n",
    "        pbar.refresh()\n",
    "\n",
    "# 处理图像 tokens\n",
    "with tqdm.tqdm(total=len(flattened_image_list), desc=\"Processing flattened_image_list\") as pbar:\n",
    "    for token in flattened_image_list:\n",
    "        for feature_index, activation_value in token['features'].items():\n",
    "            # 将激活值、token 和类型 ('image') 添加到对应特征的列表中\n",
    "            features_top[feature_index].append((activation_value, token, 'image'))\n",
    "        pbar.update(1)\n",
    "        pbar.refresh()\n",
    "\n",
    "# 对每个特征进行排序并选取激活值最高的 30 个 tokens\n",
    "with tqdm.tqdm(total=feature_num, desc=\"Processing feature_num\") as pbar:\n",
    "    for i in range(feature_num):\n",
    "        tokens_with_activation = features_top[i]\n",
    "        if tokens_with_activation:\n",
    "            # 按激活值从大到小排序\n",
    "            tokens_with_activation.sort(key=lambda x: x[0], reverse=True)\n",
    "            # 选取前 30 个激活值最高的 tokens\n",
    "            features_top[i] = tokens_with_activation[:30]\n",
    "        else:\n",
    "            # 如果没有对应的 tokens，设为空列表\n",
    "            features_top[i] = []\n",
    "        pbar.update(1)\n",
    "        pbar.refresh()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing feature_num:  48%|████▊     | 31765/65536 [00:41<00:44, 758.15it/s]\n"
     ]
    }
   ],
   "source": [
    "text_feature_list = []  # 存储所有 tokens 均为文本的特征索引\n",
    "image_feature_list = []  # 存储所有 tokens 均为图像的特征索引\n",
    "cosi_feature_list = []  # 存储同时包含文本和图像 tokens 的特征索引及其平均余弦相似度\n",
    "\n",
    "# 遍历每个特征\n",
    "with tqdm.tqdm(total=feature_num, desc=\"Processing feature_num\") as pbar:\n",
    "    for i in range(feature_num):\n",
    "        tokens = features_top[i]  # 获取特征 i 对应的 tokens 列表\n",
    "        if not tokens:\n",
    "            continue  # 如果没有 tokens，跳过该特征\n",
    "\n",
    "        # 分别存储文本和图像 tokens\n",
    "        text_tokens = []\n",
    "        image_tokens = []\n",
    "\n",
    "        for activation_value, token, token_type in tokens:\n",
    "            if token_type == 'text':\n",
    "                text_tokens.append((activation_value, token))\n",
    "            elif token_type == 'image':\n",
    "                image_tokens.append((activation_value, token))\n",
    "\n",
    "        # 判断特征类型并分类\n",
    "        if len(text_tokens) == len(tokens):\n",
    "            # 如果所有 tokens 均为文本\n",
    "            text_feature_list.append(i)\n",
    "        elif len(image_tokens) == len(tokens):\n",
    "            # 如果所有 tokens 均为图像\n",
    "            image_feature_list.append(i)\n",
    "        else:\n",
    "            # 同时包含文本和图像 tokens\n",
    "            if len(text_tokens) >= 5 and len(image_tokens) >= 5:\n",
    "                # 取前 5 个激活值最高的文本和图像 tokens\n",
    "                top_text_tokens = text_tokens[:5]\n",
    "                top_image_tokens = image_tokens[:5]\n",
    "\n",
    "                # 提取 logits\n",
    "                text_logits = [token['logits'] for _, token in top_text_tokens]\n",
    "                image_logits = [token['logits'] for _, token in top_image_tokens]\n",
    "\n",
    "                # 计算每个文本 logits 与每个图像 logits 的余弦相似度，并取平均值\n",
    "                cosine_similarities = []\n",
    "                for t_logit in text_logits:\n",
    "                    for i_logit in image_logits:\n",
    "                        # 确保 logits 是 numpy 数组\n",
    "                        t_logit = np.array(t_logit)\n",
    "                        i_logit = np.array(i_logit)\n",
    "\n",
    "                        # 计算余弦相似度\n",
    "                        numerator = np.dot(t_logit, i_logit)\n",
    "                        denominator = np.linalg.norm(t_logit) * np.linalg.norm(i_logit)\n",
    "                        if denominator == 0:\n",
    "                            cosine_similarity = 0\n",
    "                        else:\n",
    "                            cosine_similarity = numerator / denominator\n",
    "                        cosine_similarities.append(cosine_similarity)\n",
    "\n",
    "                # 计算平均余弦相似度\n",
    "                average_cosine_similarity = np.mean(cosine_similarities)\n",
    "\n",
    "                # 将特征索引和平均余弦相似度添加到列表\n",
    "                cosi_feature_list.append((i, average_cosine_similarity))\n",
    "        pbar.update(1)\n",
    "        pbar.refresh()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/mnt/file2/changye/dataset/Align-Anything-preference_interp/AA_cooccur_0_25/text_feature_list.txt', 'w') as f:\n",
    "    for feature_index in text_feature_list:\n",
    "        f.write(f\"{feature_index}\\n\")\n",
    "\n",
    "# 保存 image_feature_list\n",
    "with open('/mnt/file2/changye/dataset/Align-Anything-preference_interp/AA_cooccur_0_25/image_feature_list.txt', 'w') as f:\n",
    "    for feature_index in image_feature_list:\n",
    "        f.write(f\"{feature_index}\\n\")\n",
    "\n",
    "# 保存 cosi_feature_list\n",
    "with open('/mnt/file2/changye/dataset/Align-Anything-preference_interp/AA_cooccur_0_25/cosi_feature_list.txt', 'w') as f:\n",
    "    for feature_index, average_cosine_similarity in cosi_feature_list:\n",
    "        f.write(f\"{feature_index},{average_cosine_similarity}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
