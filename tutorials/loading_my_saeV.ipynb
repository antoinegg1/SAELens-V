{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MNk7IylTv610"
   },
   "source": [
    "# Loading and Analysing Pre-Trained Sparse Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i_DusoOvwV0M"
   },
   "source": [
    "## Imports & Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "yfDUxRx0wSRl"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab # type: ignore\n",
    "    from google.colab import output\n",
    "    COLAB = True\n",
    "    %pip install sae-lens transformer-lens\n",
    "except:\n",
    "    COLAB = False\n",
    "    from IPython import get_ipython # type: ignore\n",
    "    ipython = get_ipython(); assert ipython is not None\n",
    "    ipython.run_line_magic(\"load_ext\", \"autoreload\")\n",
    "    ipython.run_line_magic(\"autoreload\", \"2\")\n",
    "\n",
    "# Standard imports\n",
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import plotly.express as px\n",
    "\n",
    "# Imports for displaying vis in Colab / notebook\n",
    "import webbrowser\n",
    "import http.server\n",
    "import socketserver\n",
    "import threading\n",
    "PORT = 8000\n",
    "\n",
    "torch.set_grad_enabled(False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7aGgWkbav610"
   },
   "source": [
    "## Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rQSD7trbv610",
    "outputId": "222a40c4-75d4-46e2-ed3f-991841144926"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:6\n"
     ]
    }
   ],
   "source": [
    "# For the most part I'll try to import functions and classes near where they are used\n",
    "# to make it clear where they come from.\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cuda:6\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "cPUq_bdW8mcp"
   },
   "outputs": [],
   "source": [
    "def display_vis_inline(filename: str, height: int = 850):\n",
    "    '''\n",
    "    Displays the HTML files in Colab. Uses global `PORT` variable defined in prev cell, so that each\n",
    "    vis has a unique port without having to define a port within the function.\n",
    "    '''\n",
    "    if not(COLAB):\n",
    "        webbrowser.open(filename);\n",
    "\n",
    "    else:\n",
    "        global PORT\n",
    "\n",
    "        def serve(directory):\n",
    "            os.chdir(directory)\n",
    "\n",
    "            # Create a handler for serving files\n",
    "            handler = http.server.SimpleHTTPRequestHandler\n",
    "\n",
    "            # Create a socket server with the handler\n",
    "            with socketserver.TCPServer((\"\", PORT), handler) as httpd:\n",
    "                print(f\"Serving files from {directory} on port {PORT}\")\n",
    "                httpd.serve_forever()\n",
    "\n",
    "        thread = threading.Thread(target=serve, args=(\"/content\",))\n",
    "        thread.start()\n",
    "\n",
    "        output.serve_kernel_port_as_iframe(PORT, path=f\"/{filename}\", height=height, cache_in_notebook=True)\n",
    "\n",
    "        PORT += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoMx3VZpv611"
   },
   "source": [
    "# Loading a pretrained Sparse Autoencoder\n",
    "\n",
    "Below we load a Transformerlens model, a pretrained SAE and a dataset from huggingface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "sNSfL80Uv611"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "670cfca27cfd4aaaa08c48fde734893d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model llava-hf/llava-v1.6-mistral-7b-hf into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saev/changye/SAELens-V/sae_lens/sae.py:136: UserWarning: \n",
      "This SAE has non-empty model_from_pretrained_kwargs. \n",
      "For optimal performance, load the model like so:\n",
      "model = HookedSAETransformer.from_pretrained_no_processing(..., **cfg.model_from_pretrained_kwargs)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict, IterableDataset, load_dataset,load_from_disk\n",
    "from transformer_lens import HookedTransformer\n",
    "from typing import Any, Generator, Iterator, Literal, cast\n",
    "from sae_lens import SAE\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    LlavaNextForConditionalGeneration,\n",
    "    LlavaNextProcessor,\n",
    "    AutoModelForCausalLM,\n",
    ")\n",
    "\n",
    "from transformer_lens.HookedLlava import HookedLlava\n",
    "MODEL_NAME = \"llava-hf/llava-v1.6-mistral-7b-hf\"\n",
    "model_path=\"/mnt/data/changye/model/llava\"\n",
    "\n",
    "processor = LlavaNextProcessor.from_pretrained(model_path)\n",
    "vision_model = LlavaNextForConditionalGeneration.from_pretrained(\n",
    "        model_path, \n",
    "        torch_dtype=torch.float32, \n",
    "        low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "vision_tower = vision_model.vision_tower.to(\"cuda:5\")\n",
    "multi_modal_projector = vision_model.multi_modal_projector.to(\"cuda:5\")\n",
    "# 加载 HookedTransformer 语言模型\n",
    "hook_language_model = HookedLlava.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        hf_model=vision_model.language_model,\n",
    "        device=\"cuda:5\", \n",
    "        fold_ln=False,\n",
    "        center_writing_weights=False,\n",
    "        center_unembed=False,\n",
    "        tokenizer=None,\n",
    "        dtype=torch.float32,\n",
    "        vision_tower=vision_tower,\n",
    "        multi_modal_projector=multi_modal_projector,\n",
    "        n_devices=2,\n",
    "    )\n",
    "# del vision_model\n",
    "# the cfg dict is returned alongside the SAE since it may contain useful information for analysing the SAE (eg: instantiating an activation store)\n",
    "# Note that this is not the same as the SAEs config dict, rather it is whatever was in the HF repo, from which we can extract the SAE config dict\n",
    "# We also return the feature sparsities which are stored in HF for convenience. \n",
    "# sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "#     release = \"gpt2-small-res-jb\", # see other options in sae_lens/pretrained_saes.yaml\n",
    "#     sae_id = \"blocks.8.hook_resid_pre\", # won't always be a hook point\n",
    "#     device = device\n",
    "# )\n",
    "\n",
    "sae = SAE.load_from_pretrained(\n",
    "    path = \"/mnt/data/changye/checkpoints/checkpoints-V/kxpk98cr/final_122880000\",\n",
    "    device =\"cuda:7\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f681f7496ed248a5beb04eecd14a2d4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset set format\n"
     ]
    }
   ],
   "source": [
    "from transformer_lens.utils import tokenize_and_concatenate\n",
    "import transformer_lens.utils as utils\n",
    "dataset_path=\"/mnt/data/changye/data/obelics3k-tokenized-llava4096\"\n",
    "try:\n",
    "    dataset = (\n",
    "        load_dataset(\n",
    "                dataset_path,\n",
    "                split=\"train\",\n",
    "                streaming=False,\n",
    "                trust_remote_code=False,  # type: ignore\n",
    "        )\n",
    "        if isinstance(dataset_path, str)\n",
    "            else dataset_path\n",
    "        )\n",
    "except Exception as e:\n",
    "    dataset = (\n",
    "        load_from_disk(\n",
    "                dataset_path,\n",
    "            )\n",
    "            if isinstance(dataset_path, str)\n",
    "            else dataset_path\n",
    "        )\n",
    "if isinstance(dataset, (Dataset, DatasetDict)):\n",
    "        dataset = cast(Dataset | DatasetDict, dataset)\n",
    "# dataset_sample = next(iter(dataset))\n",
    "columns_to_read=[\"input_ids\",\"pixel_values\",\"attention_mask\",\"image_sizes\"]\n",
    "ds_context_size = len(dataset[\"input_ids\"])\n",
    "if hasattr(dataset, \"set_format\"):\n",
    "    dataset.set_format(type=\"torch\", columns=columns_to_read)\n",
    "    print(\"dataset set format\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "batch = dataset[:batch_size]\n",
    "batch_tokens = {\n",
    "    \"input_ids\": batch[\"input_ids\"],\n",
    "    \"pixel_values\": batch[\"pixel_values\"],\n",
    "    \"attention_mask\": batch[\"attention_mask\"],\n",
    "    \"image_sizes\": batch[\"image_sizes\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gy2uUl38v611"
   },
   "source": [
    "## Basic Analysis\n",
    "\n",
    "Let's check some basic stats on this SAE in order to see how some basic functionality in the codebase works.\n",
    "\n",
    "We'll calculate:\n",
    "- L0 (the number of features that fire per activation)\n",
    "- The cross entropy loss when the output of the SAE is used in place of the activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xOcubgsRv611"
   },
   "source": [
    "### L0 Test and Reconstruction Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SAEConfig(architecture='standard', d_in=4096, d_sae=65536, activation_fn_str='relu', apply_b_dec_to_input=False, finetuning_scaling_factor=False, context_size=4096, model_name='llava-hf/llava-v1.6-mistral-7b-hf', hook_name='blocks.16.hook_resid_post', hook_layer=16, hook_head_index=None, prepend_bos=True, dataset_path='/home/saev/changye/data/obelics100k-tokenized-llava4096_4image', dataset_trust_remote_code=True, normalize_activations='expected_average_only_in', dtype='float32', device='cuda:7', sae_lens_training_version='3.20.0', activation_fn_kwargs={}, neuronpedia_id=None, model_from_pretrained_kwargs={'n_devices': 3})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sae.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gAUR5CRBv611"
   },
   "outputs": [],
   "source": [
    "sae.eval()  # prevents error if we're expecting a dead neuron mask for who grads\n",
    "\n",
    "with torch.no_grad():\n",
    "    # activation store can give us tokens.\n",
    "    _, cache = hook_language_model.run_with_cache(batch_tokens, prepend_bos=True, names_filter=lambda name: name == sae.cfg.hook_name)\n",
    "\n",
    "    # Use the SAE\n",
    "    feature_acts = sae.encode(cache[sae.cfg.hook_name])\n",
    "    sae_out = sae.decode(feature_acts)\n",
    "\n",
    "    # save some room\n",
    "    del cache\n",
    "\n",
    "    # ignore the bos token, get the number of features that activated in each token, averaged accross batch and position\n",
    "    l0 = (feature_acts[:, 1:] > 0).float().sum(-1).detach()\n",
    "    print(\"average l0\", l0.mean().item())\n",
    "    px.histogram(l0.flatten().cpu().numpy()).show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ijoelLtdv611"
   },
   "source": [
    "Note that while the mean L0 is 64, it varies with the specific activation.\n",
    "\n",
    "To estimate reconstruction performance, we calculate the CE loss of the model with and without the SAE being used in place of the activations. This will vary depending on the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fwrSvREJv612"
   },
   "outputs": [],
   "source": [
    "from transformer_lens import utils\n",
    "from functools import partial\n",
    "torch.cuda.empty_cache()\n",
    "# next we want to do a reconstruction test.\n",
    "def reconstr_hook(activation, hook, sae_out):\n",
    "    return sae_out\n",
    "\n",
    "\n",
    "def zero_abl_hook(activation, hook):\n",
    "    return torch.zeros_like(activation)\n",
    "\n",
    "\n",
    "print(\"Orig\", hook_language_model(batch_tokens, return_type=\"loss\").item())\n",
    "print(\n",
    "    \"reconstr\",\n",
    "    hook_language_model.run_with_hooks(\n",
    "        batch_tokens,\n",
    "        fwd_hooks=[\n",
    "            (\n",
    "                sae.cfg.hook_name,\n",
    "                partial(reconstr_hook, sae_out=sae_out),\n",
    "            )\n",
    "        ],\n",
    "        return_type=\"loss\",\n",
    "    ).item(),\n",
    ")\n",
    "print(\n",
    "    \"Zero\",\n",
    "    hook_language_model.run_with_hooks(\n",
    "        batch_tokens,\n",
    "        return_type=\"loss\",\n",
    "        fwd_hooks=[(sae.cfg.hook_name, zero_abl_hook)],\n",
    "    ).item(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B_TRq_lFv612"
   },
   "source": [
    "## Specific Capability Test\n",
    "\n",
    "Validating model performance on specific tasks when using the reconstructed activation is quite important when studying specific tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "npxKip_Qv612"
   },
   "outputs": [],
   "source": [
    "example_prompt = \"The fruit in the image is \"\n",
    "example_answer = \"Apple\"\n",
    "torch.cuda.empty_cache()\n",
    "# utils.test_prompt(example_prompt, example_answer, hook_language_model, prepend_bos=True)\n",
    "from PIL import Image\n",
    "image_path=\"/home/saev/changye/TransformerLens-V/Apple.jpg\"\n",
    "image = Image.open(image_path)\n",
    "image=image.resize((336, 336))\n",
    "conversation = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": example_prompt},\n",
    "                {\"type\": \"image\"},\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "    \n",
    "    # 处理图像和文本输入\n",
    "inputs = processor(images=image, text=prompt, return_tensors=\"pt\")\n",
    "# print(inputs)\n",
    "inputs=inputs.to(\"cuda:6\")\n",
    "logits, cache = hook_language_model.run_with_cache(input=inputs,model_inputs=inputs,vision=True, prepend_bos=True)\n",
    "# inputs = hook_language_model.to_tokens(inputs)\n",
    "sae_out = sae(cache[sae.cfg.hook_name])\n",
    "\n",
    "\n",
    "\n",
    "def reconstr_hook(activations, hook, sae_out):\n",
    "    return sae_out\n",
    "\n",
    "\n",
    "def zero_abl_hook(mlp_out, hook):\n",
    "    return torch.zeros_like(mlp_out)\n",
    "\n",
    "\n",
    "hook_name = sae.cfg.hook_name\n",
    "\n",
    "print(\"Orig\", hook_language_model(inputs,model_inputs=inputs,vision=True, return_type=\"loss\").item())\n",
    "print(\n",
    "    \"reconstr\",\n",
    "    hook_language_model.run_with_hooks(\n",
    "        inputs,\n",
    "        fwd_hooks=[\n",
    "            (\n",
    "                hook_name,\n",
    "                partial(reconstr_hook, sae_out=sae_out),\n",
    "            )\n",
    "        ],\n",
    "        return_type=\"loss\",\n",
    "    ).item(),\n",
    ")\n",
    "print(\n",
    "    \"Zero\",\n",
    "    hook_language_model.run_with_hooks(\n",
    "        inputs,\n",
    "        return_type=\"loss\",\n",
    "        fwd_hooks=[(hook_name, zero_abl_hook)],\n",
    "    ).item(),\n",
    ")\n",
    "\n",
    "\n",
    "# with hook_language_model.hooks(\n",
    "#     fwd_hooks=[\n",
    "#         (\n",
    "#             hook_name,\n",
    "#             partial(reconstr_hook, sae_out=sae_out),\n",
    "#         )\n",
    "#     ]\n",
    "# ):\n",
    "#     utils.test_prompt(example_prompt, example_answer, hook_language_model, prepend_bos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [1, 415, 7212, 349, 5045, 3154, 28723], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "example_prompt=\"The sky is blue today.\"\n",
    "tokens=processor.tokenizer(example_prompt)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "edt8ag4fv612"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b0a98ded568433bbab527698d3ca680",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Forward passes to cache data for vis:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6731e30857a14c388035828c7f71c7d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting vis data from cached data:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 15\u001b[0m\n\u001b[1;32m      7\u001b[0m feature_vis_config_gpt \u001b[38;5;241m=\u001b[39m SaeVisConfig(\n\u001b[1;32m      8\u001b[0m     hook_point\u001b[38;5;241m=\u001b[39mhook_name,\n\u001b[1;32m      9\u001b[0m     features\u001b[38;5;241m=\u001b[39mtest_feature_idx_gpt,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     14\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[0;32m---> 15\u001b[0m sae_vis_data_gpt \u001b[38;5;241m=\u001b[39m \u001b[43mSaeVisData\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msae\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhook_language_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokens\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_vis_config_gpt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/changye/saeV_vis/sae_vis/data_storing_fns.py:1040\u001b[0m, in \u001b[0;36mSaeVisData.create\u001b[0;34m(cls, encoder, model, tokens, cfg, encoder_B)\u001b[0m\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m     encoder_wrapper \u001b[38;5;241m=\u001b[39m encoder\n\u001b[0;32m-> 1040\u001b[0m sae_vis_data \u001b[38;5;241m=\u001b[39m \u001b[43mget_feature_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1041\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_wrapper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1042\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1043\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1044\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1045\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_B\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_B\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1046\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1047\u001b[0m sae_vis_data\u001b[38;5;241m.\u001b[39mcfg \u001b[38;5;241m=\u001b[39m cfg\n\u001b[1;32m   1048\u001b[0m sae_vis_data\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m model\n",
      "File \u001b[0;32m~/miniconda3/envs/changyesae/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/changye/saeV_vis/sae_vis/data_fetching_fns.py:613\u001b[0m, in \u001b[0;36mget_feature_data\u001b[0;34m(encoder, model, tokens, cfg, encoder_B)\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[38;5;66;03m# For each batch of features: get new data and update global data storage objects\u001b[39;00m\n\u001b[1;32m    612\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m features \u001b[38;5;129;01min\u001b[39;00m feature_batches:\n\u001b[0;32m--> 613\u001b[0m     new_feature_data, new_time_logs \u001b[38;5;241m=\u001b[39m \u001b[43m_get_feature_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_B\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_wrapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    616\u001b[0m     sae_vis_data\u001b[38;5;241m.\u001b[39mupdate(new_feature_data)\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m new_time_logs\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/miniconda3/envs/changyesae/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/changye/saeV_vis/sae_vis/data_fetching_fns.py:465\u001b[0m, in \u001b[0;36m_get_feature_data\u001b[0;34m(encoder, encoder_B, model, tokens, feature_indices, cfg, progress)\u001b[0m\n\u001b[1;32m    459\u001b[0m     feature_indices \u001b[38;5;241m=\u001b[39m [feature_indices]\n\u001b[1;32m    461\u001b[0m \u001b[38;5;66;03m# Get tokens into minibatches, for the fwd pass\u001b[39;00m\n\u001b[1;32m    462\u001b[0m token_minibatches \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    463\u001b[0m     (tokens,)\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cfg\u001b[38;5;241m.\u001b[39mminibatch_size_tokens \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 465\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mtokens\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m(cfg\u001b[38;5;241m.\u001b[39mminibatch_size_tokens)\n\u001b[1;32m    466\u001b[0m )\n\u001b[1;32m    467\u001b[0m token_minibatches \u001b[38;5;241m=\u001b[39m [tok\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m tok \u001b[38;5;129;01min\u001b[39;00m token_minibatches]\n\u001b[1;32m    469\u001b[0m \u001b[38;5;66;03m# ! Data setup code (defining the main objects we'll eventually return, for each of 5 possible vis components)\u001b[39;00m\n\u001b[1;32m    470\u001b[0m \n\u001b[1;32m    471\u001b[0m \u001b[38;5;66;03m# Create lists to store the feature activations & final values of the residual stream\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "from sae_vis.data_config_classes import SaeVisConfig\n",
    "from sae_vis.data_storing_fns import SaeVisData\n",
    "torch.cuda.empty_cache()\n",
    "sae.eval()\n",
    "test_feature_idx_gpt = list(range(10)) + [14057]\n",
    "hook_name = sae.cfg.hook_name\n",
    "feature_vis_config_gpt = SaeVisConfig(\n",
    "    hook_point=hook_name,\n",
    "    features=test_feature_idx_gpt,\n",
    "    # batch_size=2048,\n",
    "    minibatch_size_tokens=128,\n",
    "    verbose=True,\n",
    ")\n",
    "torch.cuda.empty_cache()\n",
    "sae_vis_data_gpt = SaeVisData.create(\n",
    "    encoder=sae,\n",
    "    model=hook_language_model, # type: ignore\n",
    "    tokens=tokens['input_ids'],  # type: ignore\n",
    "    cfg=feature_vis_config_gpt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yQ94Frzbv612"
   },
   "outputs": [],
   "source": [
    "for feature in test_feature_idx_gpt:\n",
    "    filename = f\"{feature}_feature_vis_demo_gpt.html\"\n",
    "    sae_vis_data_gpt.save_feature_centric_vis(filename, feature)\n",
    "    display_vis_inline(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AUaD6CFDv612"
   },
   "source": [
    "Now, since generating feature dashboards can be done once per sparse autoencoder, for pre-trained SAEs in the public domain, everyone can use the same dashboards. Neuronpedia hosts dashboards which we can load via the intergration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BxluyNRBv612"
   },
   "outputs": [],
   "source": [
    "# from sae_lens.analysis.neuronpedia_integration import get_neuronpedia_quick_list\n",
    "\n",
    "# # this function should open\n",
    "# neuronpedia_quick_list = get_neuronpedia_quick_list(\n",
    "#     sae=sae,\n",
    "#     features=test_feature_idx_gpt,\n",
    "#     # layer=sae.cfg.hook_layer,\n",
    "#     # model=\"llava-hf/llava-v1.6-mistral-7b-hf\",\n",
    "#     # dataset=\"res-jb\",\n",
    "\n",
    "#     name=\"A quick list we made\",\n",
    "# )\n",
    "\n",
    "# if COLAB:\n",
    "#   # If you're on colab, click the link below\n",
    "#   print(neuronpedia_quick_list)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "changyesae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
