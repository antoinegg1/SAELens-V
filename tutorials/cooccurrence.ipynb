{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab # type: ignore\n",
    "    from google.colab import output\n",
    "    COLAB = True\n",
    "    %pip install sae-lens transformer-lens\n",
    "except:\n",
    "    COLAB = False\n",
    "    from IPython import get_ipython # type: ignore\n",
    "    ipython = get_ipython(); assert ipython is not None\n",
    "    ipython.run_line_magic(\"load_ext\", \"autoreload\")\n",
    "    ipython.run_line_magic(\"autoreload\", \"2\")\n",
    "\n",
    "# Standard imports\n",
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import plotly.express as px  \n",
    "import random\n",
    "from datasets import Dataset, DatasetDict, IterableDataset, load_dataset,load_from_disk\n",
    "from transformer_lens import HookedTransformer\n",
    "from typing import Any, Generator, Iterator, Literal, cast\n",
    "from sae_lens import SAE\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    LlavaNextForConditionalGeneration,\n",
    "    LlavaNextProcessor,\n",
    "    AutoModelForCausalLM,\n",
    ")\n",
    "os.environ[\"http_proxy\"] = \"http://127.0.0.1:7890\"\n",
    "os.environ[\"https_proxy\"] = \"http://127.0.0.1:7890\"\n",
    "\n",
    "from transformer_lens.HookedLlava import HookedLlava\n",
    "from sae_lens.activation_visualization import (\n",
    "    load_llava_model,\n",
    "    load_sae,\n",
    "    separate_feature,\n",
    "    run_model,\n",
    ")\n",
    "model_name = \"llava-hf/llava-v1.6-mistral-7b-hf\"\n",
    "model_path=\"/data/models/llava-v1.6-mistral-7b-hf\"\n",
    "sae_path=\"/data/changye/model/llavasae_obliec100k_SAEV\"\n",
    "sae_device=\"cuda:7\"\n",
    "device=\"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # 加载模型\n",
    "processor,  hook_language_model = load_llava_model(\n",
    "        model_name, model_path, device,n_devices=7\n",
    "    )\n",
    "sae = load_sae(sae_path, sae_device)\n",
    "# del vision_model\n",
    "# the cfg dict is returned alongside the SAE since it may contain useful information for analysing the SAE (eg: instantiating an activation store)\n",
    "# Note that this is not the same as the SAEs config dict, rather it is whatever was in the HF repo, from which we can extract the SAE config dict\n",
    "# We also return the feature sparsities which are stored in HF for convenience. \n",
    "# sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "#     release = \"gpt2-small-res-jb\", # see other options in sae_lens/pretrained_saes.yaml\n",
    "#     sae_id = \"blocks.8.hook_resid_pre\", # won't always be a hook point\n",
    "#     device = device\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path=\"/data/changye/data/SPA-VL\"\n",
    "system_prompt= \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. \"\n",
    "user_prompt= 'USER: \\n<image> {input}'\n",
    "assistant_prompt= '\\nASSISTANT: {output}'\n",
    "split_token= 'ASSISTANT:'\n",
    "train_dataset = load_dataset(\n",
    "            dataset_path,\n",
    "            split=\"train\",\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "print(train_dataset)\n",
    "sample_size = 2\n",
    "total_size = len(train_dataset)\n",
    "random_indices = random.sample(range(total_size), sample_size)\n",
    "sampled_dataset = train_dataset.select(random_indices)\n",
    "\n",
    "# 定义格式化函数\n",
    "def format_sample(raw_sample: dict[str, Any]) -> dict[str, Any]:\n",
    "    \"\"\"\n",
    "    格式化样本，只提取 question 和 image 字段，并生成所需的 prompt。\n",
    "    \"\"\"\n",
    "    # 获取并清洗 question 字段\n",
    "    prompt = raw_sample['question'].replace('<image>\\n', '').replace('\\n<image>', '').replace('<image>', '')\n",
    "    \n",
    "    # 加载和处理 image 字段\n",
    "    image = raw_sample['image']\n",
    "    # if isinstance(image, str):  # 如果 image 是路径\n",
    "    #     image = Image.open(image).convert('RGBA')\n",
    "    # elif hasattr(image, \"convert\"):  # 如果是 PIL.Image 对象\n",
    "    image = image.convert('RGBA')\n",
    "\n",
    "    \n",
    "    # 格式化 Prompt\n",
    "    formatted_prompt = (\n",
    "        f'{system_prompt}'\n",
    "        f'{user_prompt.format(input=prompt)}'\n",
    "        f'{assistant_prompt.format(output=\"\")}'\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'prompt': formatted_prompt,\n",
    "        'image': image,\n",
    "    }\n",
    "\n",
    "# 使用 map 方法处理数据集\n",
    "formatted_dataset = sampled_dataset.map(\n",
    "    format_sample,\n",
    "    num_proc=80,  # 根据您的 CPU 核心数量调整\n",
    "    remove_columns=['chosen','rejected','image_name','question'],\n",
    ")\n",
    "print(formatted_dataset)\n",
    "# 如果需要进一步处理，可以将 formatted_dataset 转换为列表\n",
    "formatted_sample = formatted_dataset[:]\n",
    "print(formatted_sample['prompt'][0])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(\n",
    "        text=formatted_sample['prompt'],\n",
    "        images=formatted_sample['image'],\n",
    "        return_tensors='pt',\n",
    "        padding='max_length',  # 设置padding为最大长度\n",
    "        max_length=256,  # 设置最大长度\n",
    "    ).to(device)\n",
    "\n",
    "# 打印一个处理后的示例\n",
    "print((inputs['input_ids'].shape))\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2506, 65536])\n"
     ]
    }
   ],
   "source": [
    "# for batch in processed_dataset:\n",
    "#     # print(dir(batch))\n",
    "#     image_indices, feature_act = run_model(batch, hook_language_model, sae, sae_device)\n",
    "#     break  \n",
    "\n",
    "image_indices, feature_act = run_model(inputs, hook_language_model, sae, sae_device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2506, 32064])\n",
      "torch.Size([2, 2506, 65536])\n"
     ]
    }
   ],
   "source": [
    "print((image_indices[0].shape))\n",
    "print(feature_act.shape)\n",
    "\n",
    "\n",
    "### 11.28 problem! 追踪image_indice的变化(尤其是在hookllava中)，目前状态异常"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cooccurrence_feature=separate_feature(image_indices, feature_act)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
