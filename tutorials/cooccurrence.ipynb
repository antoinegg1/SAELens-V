{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import google.colab # type: ignore\n",
    "    from google.colab import output\n",
    "    COLAB = True\n",
    "    %pip install sae-lens transformer-lens\n",
    "except:\n",
    "    COLAB = False\n",
    "    from IPython import get_ipython # type: ignore\n",
    "    ipython = get_ipython(); assert ipython is not None\n",
    "    ipython.run_line_magic(\"load_ext\", \"autoreload\")\n",
    "    ipython.run_line_magic(\"autoreload\", \"2\")\n",
    "\n",
    "# Standard imports\n",
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import plotly.express as px  \n",
    "import random\n",
    "from datasets import Dataset, DatasetDict, IterableDataset, load_dataset,load_from_disk\n",
    "from transformer_lens import HookedTransformer\n",
    "from typing import Any, Generator, Iterator, Literal, cast\n",
    "from sae_lens import SAE\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    LlavaNextForConditionalGeneration,\n",
    "    LlavaNextProcessor,\n",
    "    AutoModelForCausalLM,\n",
    ")\n",
    "os.environ[\"http_proxy\"] = \"http://127.0.0.1:7890\"\n",
    "os.environ[\"https_proxy\"] = \"http://127.0.0.1:7890\"\n",
    "\n",
    "from transformer_lens.HookedLlava import HookedLlava\n",
    "from sae_lens.activation_visualization import (\n",
    "    load_llava_model,\n",
    "    load_sae,\n",
    "    separate_feature,\n",
    ")\n",
    "model_name = \"llava-hf/llava-v1.6-mistral-7b-hf\"\n",
    "model_path=\"/data/models/llava-v1.6-mistral-7b-hf\"\n",
    "sae_path=\"/data/changye/model/llavasae_obliec100k_SAEV\"\n",
    "sae_device=\"cuda:7\"\n",
    "device=\"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # 加载模型\n",
    "processor, vision_model, vision_tower, multi_modal_projector, hook_language_model = load_llava_model(\n",
    "        model_name, model_path, device,n_devices=8\n",
    "    )\n",
    "sae = load_sae(sae_path, sae_device)\n",
    "# del vision_model\n",
    "# the cfg dict is returned alongside the SAE since it may contain useful information for analysing the SAE (eg: instantiating an activation store)\n",
    "# Note that this is not the same as the SAEs config dict, rather it is whatever was in the HF repo, from which we can extract the SAE config dict\n",
    "# We also return the feature sparsities which are stored in HF for convenience. \n",
    "# sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "#     release = \"gpt2-small-res-jb\", # see other options in sae_lens/pretrained_saes.yaml\n",
    "#     sae_id = \"blocks.8.hook_resid_pre\", # won't always be a hook point\n",
    "#     device = device\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_proc must be <= 8. Reducing num_proc to 8 for dataset of size 8.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['image', 'question', 'chosen', 'rejected', 'image_name'],\n",
      "    num_rows: 93258\n",
      "})\n",
      "[27/Nov/2024 13:49:29] WARNING - num_proc must be <= 8. Reducing num_proc to 8 for dataset of size 8.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88b9187384cc412a80572f8c43929781",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/8 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['image', 'prompt'],\n",
      "    num_rows: 8\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset_path=\"/data/changye/data/SPA-VL\"\n",
    "system_prompt= \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. \"\n",
    "user_prompt= 'USER: \\n<image> {input}'\n",
    "assistant_prompt= '\\nASSISTANT: {output}'\n",
    "split_token= 'ASSISTANT:'\n",
    "train_dataset = load_dataset(\n",
    "            dataset_path,\n",
    "            split=\"train\",\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "print(train_dataset)\n",
    "sample_size = 8\n",
    "total_size = len(train_dataset)\n",
    "random_indices = random.sample(range(total_size), sample_size)\n",
    "sampled_dataset = train_dataset.select(random_indices)\n",
    "\n",
    "# 定义格式化函数\n",
    "def format_sample(raw_sample: dict[str, Any]) -> dict[str, Any]:\n",
    "    \"\"\"\n",
    "    格式化样本，只提取 question 和 image 字段，并生成所需的 prompt。\n",
    "    \"\"\"\n",
    "    # 获取并清洗 question 字段\n",
    "    prompt = raw_sample['question'].replace('<image>\\n', '').replace('\\n<image>', '').replace('<image>', '')\n",
    "    \n",
    "    # 加载和处理 image 字段\n",
    "    image = raw_sample['image']\n",
    "    # if isinstance(image, str):  # 如果 image 是路径\n",
    "    #     image = Image.open(image).convert('RGBA')\n",
    "    # elif hasattr(image, \"convert\"):  # 如果是 PIL.Image 对象\n",
    "    image = image.convert('RGBA')\n",
    "\n",
    "    \n",
    "    # 格式化 Prompt\n",
    "    formatted_prompt = (\n",
    "        f'{system_prompt}'\n",
    "        f'{user_prompt.format(input=prompt)}'\n",
    "        f'{assistant_prompt.format(output=\"\")}'\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'prompt': formatted_prompt,\n",
    "        'image': image,\n",
    "    }\n",
    "\n",
    "# 使用 map 方法处理数据集\n",
    "formatted_dataset = sampled_dataset.map(\n",
    "    format_sample,\n",
    "    num_proc=80,  # 根据您的 CPU 核心数量调整\n",
    "    remove_columns=['chosen','rejected','image_name','question'],\n",
    ")\n",
    "print(formatted_dataset)\n",
    "# 如果需要进一步处理，可以将 formatted_dataset 转换为列表\n",
    "formatted_sample = formatted_dataset[:]\n",
    "# print(formatted_sample)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    images = examples['image']\n",
    "    texts = examples['prompt']\n",
    "    inputs = processor(images=images, text=texts, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "    return inputs\n",
    "\n",
    "# 使用 processor 对数据集进行批处理\n",
    "processed_dataset = formatted_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    batch_size=8,  # 根据您的显存和需求调整\n",
    "    remove_columns=formatted_dataset.column_names,\n",
    ")\n",
    "\n",
    "# 打印一个处理后的示例\n",
    "for batch in processed_dataset:\n",
    "    print(batch)\n",
    "    break  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
