{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/aifs4su/yaodong/miniconda3/envs/lcy_interp/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import google.colab # type: ignore\n",
    "    from google.colab import output\n",
    "    COLAB = True\n",
    "    %pip install sae-lens transformer-lens\n",
    "except:\n",
    "    COLAB = False\n",
    "    from IPython import get_ipython # type: ignore\n",
    "    ipython = get_ipython(); assert ipython is not None\n",
    "    ipython.run_line_magic(\"load_ext\", \"autoreload\")\n",
    "    ipython.run_line_magic(\"autoreload\", \"2\")\n",
    "\n",
    "# Standard imports\n",
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import plotly.express as px  \n",
    "import random\n",
    "from datasets import Dataset, DatasetDict, IterableDataset, load_dataset,load_from_disk\n",
    "from transformer_lens import HookedTransformer\n",
    "from typing import Any, Generator, Iterator, Literal, cast\n",
    "from sae_lens import SAE\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    LlavaNextForConditionalGeneration,\n",
    "    LlavaNextProcessor,\n",
    "    AutoModelForCausalLM,\n",
    ")\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "\n",
    "from transformer_lens.HookedLlava import HookedLlava\n",
    "from sae_lens.activation_visualization import (\n",
    "    load_llava_model,\n",
    "    load_sae,\n",
    "    separate_feature,\n",
    "    run_model,\n",
    ")\n",
    "# os.environ[\"TMP_DIR\"]=\"/home/yaodong/tmp\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6,7\" \n",
    "model_name = \"llava-hf/llava-v1.6-mistral-7b-hf\"\n",
    "model_path=\"/data/models/llava-v1.6-mistral-7b-hf\"\n",
    "sae_path=\"/data/changye/model/llavasae_obliec100k_SAEV\"\n",
    "sae_device=\"cuda:1\"\n",
    "device=\"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # 加载模型\n",
    "processor,  hook_language_model = load_llava_model(\n",
    "        model_name, model_path, device,n_devices=2,stop_at_layer=17\n",
    "    )\n",
    "sae = load_sae(sae_path, sae_device)\n",
    "# del vision_model\n",
    "# the cfg dict is returned alongside the SAE since it may contain useful information for analysing the SAE (eg: instantiating an activation store)\n",
    "# Note that this is not the same as the SAEs config dict, rather it is whatever was in the HF repo, from which we can extract the SAE config dict\n",
    "# We also return the feature sparsities which are stored in HF for convenience. \n",
    "# sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "#     release = \"gpt2-small-res-jb\", # see other options in sae_lens/pretrained_saes.yaml\n",
    "#     sae_id = \"blocks.8.hook_resid_pre\", # won't always be a hook point\n",
    "#     device = device\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 216462 examples [00:00, 249622.62 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'image', 'conversations'],\n",
      "    num_rows: 216462\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"/aifs4su/yaodong/changye/tmp\"\n",
    "dataset_path=\"/aifs4su/yaodong/hantao/datasets/MMInstruct-GPT4V\"\n",
    "# system_prompt= \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. \"\n",
    "# user_prompt= 'USER: \\n<image> {input}'\n",
    "# assistant_prompt= '\\nASSISTANT: {output}'\n",
    "# split_token= 'ASSISTANT:'\n",
    "train_dataset = load_dataset(\n",
    "            dataset_path,\n",
    "            'qa_en',\n",
    "            split=\"train\",\n",
    "            trust_remote_code=True,\n",
    "            cache_dir=\"/aifs4su/yaodong/changye/tmp\"\n",
    "        )\n",
    "print(train_dataset)\n",
    "sample_size = 1000\n",
    "total_size = len(train_dataset)\n",
    "random_indices = random.sample(range(total_size), sample_size)\n",
    "sampled_dataset = train_dataset.select(random_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "social_relation/0001/00000770.jpg\n",
      "<PIL.Image.Image image mode=RGBA size=336x336 at 0x155137B0DE90>\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import io\n",
    "from io import BytesIO\n",
    "print(train_dataset[0]['image'])\n",
    "image = Image.open(\"/aifs4su/yaodong/changye/images/\"+train_dataset[0]['image'])\n",
    "image = image.resize((336, 336)).convert('RGBA')\n",
    "print(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 定义格式化函数\n",
    "def format_sample(raw_sample: dict[str, Any]) -> dict[str, Any]:\n",
    "    \"\"\"\n",
    "    格式化样本，只提取 question 和 image 字段，并生成所需的 prompt。\n",
    "    \"\"\"\n",
    "    # 获取并清洗 question 字段\n",
    "    prompt = raw_sample['question'].replace('<image>\\n', '').replace('\\n<image>', '').replace('<image>', '')\n",
    "    \n",
    "    # 加载和处理 image 字段\n",
    "    image = raw_sample['image']\n",
    "    # if isinstance(image, str):  # 如果 image 是路径\n",
    "    #     image = Image.open(image).convert('RGBA')\n",
    "    # elif hasattr(image, \"convert\"):  # 如果是 PIL.Image 对象\n",
    "    image=image.resize((336,336))\n",
    "    image = image.convert('RGBA')\n",
    "\n",
    "    \n",
    "    # 格式化 Prompt\n",
    "    formatted_prompt = (\n",
    "        f'{system_prompt}'\n",
    "        f'{user_prompt.format(input=prompt)}'\n",
    "        f'{assistant_prompt.format(output=\"\")}'\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'prompt': formatted_prompt,\n",
    "        'image': image,\n",
    "        'image_name':raw_sample['image_name']\n",
    "    }\n",
    "\n",
    "# 使用 map 方法处理数据集\n",
    "formatted_dataset = sampled_dataset.map(\n",
    "    format_sample,\n",
    "    num_proc=80,  # 根据您的 CPU 核心数量调整\n",
    "    remove_columns=['chosen','rejected','question'],\n",
    ")\n",
    "# print(formatted_dataset)\n",
    "# 如果需要进一步处理，可以将 formatted_dataset 转换为列表\n",
    "formatted_sample = formatted_dataset[:]\n",
    "# print(formatted_sample['image_name'][0])\n",
    "\n",
    "hf_dataset = Dataset.from_dict(formatted_sample)\n",
    "\n",
    "# 保存为 Arrow 格式\n",
    "save_path = \"/data/changye/data/SPA_VL1k\"\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "hf_dataset.save_to_disk(save_path)\n",
    "print(f\"Dataset saved to {save_path}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_name_list=[]\n",
    "\n",
    "# for data in tqdm(train_dataset):\n",
    "#     image_name=data['image_name']\n",
    "#     if image_name in image_name_list:\n",
    "#         print(\"error!\")\n",
    "#         break\n",
    "#     else:\n",
    "#         image_name_list.append(image_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(\n",
    "        text=formatted_sample['prompt'],\n",
    "        images=formatted_sample['image'],\n",
    "        return_tensors='pt',\n",
    "        padding='max_length',  # 设置padding为最大长度\n",
    "        max_length=256,  # 设置最大长度\n",
    "    ).to(device)\n",
    "\n",
    "# 打印一个处理后的示例\n",
    "print((inputs['input_ids'].shape))\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in processed_dataset:\n",
    "#     # print(dir(batch))\n",
    "#     image_indices, feature_act = run_model(batch, hook_language_model, sae, sae_device)\n",
    "#     break  \n",
    "\n",
    "image_indices, feature_act = run_model(inputs, hook_language_model, sae, sae_device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((image_indices.shape))\n",
    "print(feature_act.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cooccurrence_feature=separate_feature(image_indices, feature_act)\n",
    "print(len(cooccurrence_feature[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict={}\n",
    "for i in range(len(cooccurrence_feature)):\n",
    "    data_dict[formatted_sample['image_name'][i]]=cooccurrence_feature[i]\n",
    "print(data_dict)\n",
    "batch_size = 10000\n",
    "for i in range(0, len(data_dict), batch_size):\n",
    "    batch_dict = dict(list(data_dict.items())[i:i+batch_size])\n",
    "    torch.save(batch_dict, f'data_batch_{i // batch_size}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "train_dataset = load_dataset('PKU-Alignment/Align-Anything',name='text-image-to-text',cache_dir=\"/mnt/file2/changye/dataset/Align-Anything_preference\")['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lcy_interp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
