{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab # type: ignore\n",
    "    from google.colab import output\n",
    "    COLAB = True\n",
    "    %pip install sae-lens transformer-lens\n",
    "except:\n",
    "    COLAB = False\n",
    "    from IPython import get_ipython # type: ignore\n",
    "    ipython = get_ipython(); assert ipython is not None\n",
    "    ipython.run_line_magic(\"load_ext\", \"autoreload\")\n",
    "    ipython.run_line_magic(\"autoreload\", \"2\")\n",
    "\n",
    "# Standard imports\n",
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import plotly.express as px  \n",
    "import random\n",
    "from datasets import Dataset, DatasetDict, IterableDataset, load_dataset,load_from_disk\n",
    "from transformer_lens import HookedTransformer\n",
    "from typing import Any, Generator, Iterator, Literal, cast\n",
    "from sae_lens import SAE\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    LlavaNextForConditionalGeneration,\n",
    "    LlavaNextProcessor,\n",
    "    AutoModelForCausalLM,\n",
    ")\n",
    "os.environ[\"http_proxy\"] = \"http://127.0.0.1:7890\"\n",
    "os.environ[\"https_proxy\"] = \"http://127.0.0.1:7890\"\n",
    "\n",
    "from transformer_lens.HookedLlava import HookedLlava\n",
    "from sae_lens.activation_visualization import (\n",
    "    load_llava_model,\n",
    "    load_sae,\n",
    "    separate_feature,\n",
    "    run_model,\n",
    ")\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6,7\" \n",
    "model_name = \"llava-hf/llava-v1.6-mistral-7b-hf\"\n",
    "model_path=\"/data/models/llava-v1.6-mistral-7b-hf\"\n",
    "sae_path=\"/data/changye/model/llavasae_obliec100k_SAEV\"\n",
    "sae_device=\"cuda:1\"\n",
    "device=\"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2064a433b0174f3ea27b38d51b3379e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantization check time: 0.00s\n",
      "Configuration loading time: 0.00s\n",
      "Model configuration processing time: 1.83s\n",
      "State dict loading time: 0.01s\n",
      "Tokenizer setup time: 0.38s\n",
      "Embedding setup time: 79.95s\n",
      "Move device time: 0.00s\n",
      "Set up time: 0.00s\n",
      "Model creation time: 80.34s\n",
      "State dict processing time: 37.76s\n",
      "Device moving time: 26.25s\n",
      "Total loading time: 146.18s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/changye/SAELens-V/sae_lens/sae.py:136: UserWarning: \n",
      "This SAE has non-empty model_from_pretrained_kwargs. \n",
      "For optimal performance, load the model like so:\n",
      "model = HookedSAETransformer.from_pretrained_no_processing(..., **cfg.model_from_pretrained_kwargs)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    " # 加载模型\n",
    "processor,  hook_language_model = load_llava_model(\n",
    "        model_name, model_path, device,n_devices=2,stop_at_layer=17\n",
    "    )\n",
    "sae = load_sae(sae_path, sae_device)\n",
    "# del vision_model\n",
    "# the cfg dict is returned alongside the SAE since it may contain useful information for analysing the SAE (eg: instantiating an activation store)\n",
    "# Note that this is not the same as the SAEs config dict, rather it is whatever was in the HF repo, from which we can extract the SAE config dict\n",
    "# We also return the feature sparsities which are stored in HF for convenience. \n",
    "# sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "#     release = \"gpt2-small-res-jb\", # see other options in sae_lens/pretrained_saes.yaml\n",
    "#     sae_id = \"blocks.8.hook_resid_pre\", # won't always be a hook point\n",
    "#     device = device\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db95de06b49f426bb0754875b28ce7af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/93258 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['image', 'question', 'chosen', 'rejected', 'image_name'],\n",
      "    num_rows: 93258\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd288e7ac01e40ee87936a9d9796e902",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=80):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00be67bd87914420aa6595f72b32ca35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to /data/changye/data/SPA_VL1k\n"
     ]
    }
   ],
   "source": [
    "dataset_path=\"/data/changye/data/SPA-VL\"\n",
    "system_prompt= \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. \"\n",
    "user_prompt= 'USER: \\n<image> {input}'\n",
    "assistant_prompt= '\\nASSISTANT: {output}'\n",
    "split_token= 'ASSISTANT:'\n",
    "train_dataset = load_dataset(\n",
    "            dataset_path,\n",
    "            split=\"train\",\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "print(train_dataset)\n",
    "sample_size = 1000\n",
    "total_size = len(train_dataset)\n",
    "random_indices = random.sample(range(total_size), sample_size)\n",
    "sampled_dataset = train_dataset.select(random_indices)\n",
    "\n",
    "# 定义格式化函数\n",
    "def format_sample(raw_sample: dict[str, Any]) -> dict[str, Any]:\n",
    "    \"\"\"\n",
    "    格式化样本，只提取 question 和 image 字段，并生成所需的 prompt。\n",
    "    \"\"\"\n",
    "    # 获取并清洗 question 字段\n",
    "    prompt = raw_sample['question'].replace('<image>\\n', '').replace('\\n<image>', '').replace('<image>', '')\n",
    "    \n",
    "    # 加载和处理 image 字段\n",
    "    image = raw_sample['image']\n",
    "    # if isinstance(image, str):  # 如果 image 是路径\n",
    "    #     image = Image.open(image).convert('RGBA')\n",
    "    # elif hasattr(image, \"convert\"):  # 如果是 PIL.Image 对象\n",
    "    image=image.resize((336,336))\n",
    "    image = image.convert('RGBA')\n",
    "\n",
    "    \n",
    "    # 格式化 Prompt\n",
    "    formatted_prompt = (\n",
    "        f'{system_prompt}'\n",
    "        f'{user_prompt.format(input=prompt)}'\n",
    "        f'{assistant_prompt.format(output=\"\")}'\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'prompt': formatted_prompt,\n",
    "        'image': image,\n",
    "        'image_name':raw_sample['image_name']\n",
    "    }\n",
    "\n",
    "# 使用 map 方法处理数据集\n",
    "formatted_dataset = sampled_dataset.map(\n",
    "    format_sample,\n",
    "    num_proc=80,  # 根据您的 CPU 核心数量调整\n",
    "    remove_columns=['chosen','rejected','question'],\n",
    ")\n",
    "# print(formatted_dataset)\n",
    "# 如果需要进一步处理，可以将 formatted_dataset 转换为列表\n",
    "formatted_sample = formatted_dataset[:]\n",
    "# print(formatted_sample['image_name'][0])\n",
    "\n",
    "hf_dataset = Dataset.from_dict(formatted_sample)\n",
    "\n",
    "# 保存为 Arrow 格式\n",
    "save_path = \"/data/changye/data/SPA_VL1k\"\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "hf_dataset.save_to_disk(save_path)\n",
    "print(f\"Dataset saved to {save_path}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_name_list=[]\n",
    "\n",
    "# for data in tqdm(train_dataset):\n",
    "#     image_name=data['image_name']\n",
    "#     if image_name in image_name_list:\n",
    "#         print(\"error!\")\n",
    "#         break\n",
    "#     else:\n",
    "#         image_name_list.append(image_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "inputs = processor(\n",
    "        text=formatted_sample['prompt'],\n",
    "        images=formatted_sample['image'],\n",
    "        return_tensors='pt',\n",
    "        padding='max_length',  # 设置padding为最大长度\n",
    "        max_length=256,  # 设置最大长度\n",
    "    ).to(device)\n",
    "\n",
    "# 打印一个处理后的示例\n",
    "print((inputs['input_ids'].shape))\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  35,   36,   37,  ..., 1208, 1209, 1210],\n",
      "        [  35,   36,   37,  ..., 1208, 1209, 1210]], device='cuda:0')\n",
      "out (tensor([[[ -4.8597,  -4.7012,  -0.1998,  ...,   0.1763,   0.1783,   0.1782],\n",
      "         [ -6.7856,  -6.8383,  -3.4369,  ...,   0.1607,   0.1632,   0.1608],\n",
      "         [ -7.6597,  -8.0547,  -2.2168,  ...,   0.2865,   0.2930,   0.2836],\n",
      "         ...,\n",
      "         [ -8.4231,  -8.8412,   3.6364,  ...,   0.1510,   0.1481,   0.1504],\n",
      "         [ -6.9022,  -7.0688,   1.6313,  ...,  -0.0247,  -0.0260,  -0.0219],\n",
      "         [ -6.4647,  -6.3016,  10.9200,  ...,   0.2390,   0.2362,   0.2436]],\n",
      "\n",
      "        [[ -4.8597,  -4.7012,  -0.1998,  ...,   0.1763,   0.1783,   0.1782],\n",
      "         [ -6.7856,  -6.8383,  -3.4369,  ...,   0.1607,   0.1632,   0.1608],\n",
      "         [ -7.6597,  -8.0547,  -2.2168,  ...,   0.2865,   0.2930,   0.2836],\n",
      "         ...,\n",
      "         [-10.2070, -10.4642,   7.1662,  ...,   0.1577,   0.1569,   0.1592],\n",
      "         [ -6.9841,  -7.2379,   3.6436,  ...,  -0.0236,  -0.0251,  -0.0194],\n",
      "         [ -6.6523,  -6.7415,  12.9587,  ...,   0.1408,   0.1406,   0.1472]]],\n",
      "       device='cuda:7'), tensor([[  35,   36,   37,  ..., 1208, 1209, 1210],\n",
      "        [  35,   36,   37,  ..., 1208, 1209, 1210]], device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "# for batch in processed_dataset:\n",
    "#     # print(dir(batch))\n",
    "#     image_indices, feature_act = run_model(batch, hook_language_model, sae, sae_device)\n",
    "#     break  \n",
    "\n",
    "image_indices, feature_act = run_model(inputs, hook_language_model, sae, sae_device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1176])\n",
      "torch.Size([2, 1244, 65536])\n"
     ]
    }
   ],
   "source": [
    "print((image_indices.shape))\n",
    "print(feature_act.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "630\n"
     ]
    }
   ],
   "source": [
    "cooccurrence_feature=separate_feature(image_indices, feature_act)\n",
    "print(len(cooccurrence_feature[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'5655.jpg': [40963, 18436, 32792, 28699, 41002, 10285, 59445, 10293, 18488, 57401, 47165, 53326, 38998, 30808, 6238, 28768, 24673, 59489, 34919, 51305, 107, 8302, 4212, 39029, 39031, 49278, 36995, 55428, 49287, 2205, 24737, 26786, 49316, 41139, 4276, 47291, 200, 24785, 43217, 24787, 2262, 14553, 219, 26850, 26854, 49389, 8430, 63725, 63742, 26882, 47366, 18696, 28942, 20750, 30992, 59673, 6439, 6448, 57653, 59701, 47424, 43330, 47426, 49482, 14684, 63837, 35167, 8549, 49509, 55653, 10607, 12668, 6524, 8574, 63868, 386, 39298, 61829, 2454, 57754, 37293, 16831, 53696, 22979, 6603, 16864, 20963, 27117, 55790, 495, 29181, 33278, 57860, 39436, 41502, 37410, 4645, 57900, 37430, 62007, 2618, 55876, 59982, 29271, 605, 21092, 4718, 17007, 47727, 51833, 55933, 43657, 43658, 57998, 21135, 2705, 31378, 43689, 62125, 29372, 33489, 13011, 43732, 60116, 41688, 41695, 35557, 6885, 2791, 37607, 47845, 39660, 751, 13039, 6897, 47864, 17145, 17146, 47872, 33539, 51977, 21257, 25357, 25358, 23309, 37666, 806, 62250, 6967, 64321, 4939, 58196, 6996, 64341, 50009, 7005, 47970, 58212, 37737, 23406, 50031, 64367, 19313, 35702, 58240, 37761, 52098, 50051, 11142, 31626, 48017, 35735, 920, 19352, 58268, 41888, 62378, 45996, 45998, 37829, 50134, 29659, 43996, 19426, 11243, 62455, 58376, 54284, 23564, 17422, 23569, 25642, 25657, 52289, 46149, 9286, 52315, 3165, 44132, 17516, 33901, 29808, 48240, 1142, 38008, 33920, 1154, 7298, 46217, 56459, 7314, 23713, 54435, 5286, 33961, 19625, 48311, 56521, 25807, 44241, 23763, 15571, 62680, 5342, 58601, 56563, 21750, 34049, 9474, 27906, 42253, 21774, 5391, 60691, 60692, 64789, 54555, 23845, 60711, 17707, 44333, 13614, 3376, 38197, 32054, 60740, 11589, 3398, 7492, 5450, 27981, 9550, 19790, 60756, 54631, 50540, 50542, 32115, 40326, 52615, 25998, 19864, 60826, 54684, 17823, 58784, 1442, 9634, 23975, 23979, 11696, 5555, 17846, 60864, 3523, 54729, 24013, 58832, 50643, 60883, 42462, 60894, 54753, 28148, 17909, 24060, 38402, 5640, 34317, 24084, 5656, 56864, 28203, 38453, 60982, 11840, 44619, 20044, 48731, 26222, 59012, 65170, 65174, 46749, 34463, 59044, 36523, 28335, 9905, 38583, 3768, 52927, 38595, 42697, 1755, 50916, 1765, 57072, 14068, 20222, 34562, 52999, 28426, 30485, 36641, 20263, 57128, 40764, 10045, 20296, 59210, 20300, 65358, 65359, 51024, 44882, 61279, 32607, 63331, 5988, 38762, 53106, 6023, 55179, 20365, 16278, 6043, 44969, 53168, 55223, 18362, 47034, 1987, 36803, 10197, 63448, 45020, 34786, 55266, 8170, 28654, 63485], '34174.jpg': [40963, 18436, 2053, 40973, 55317, 18454, 12312, 14373, 30760, 34857, 12331, 10285, 36909, 59445, 57401, 47165, 12351, 34880, 6209, 8259, 55366, 32840, 49227, 59467, 49229, 4172, 53326, 26714, 18523, 6238, 24673, 32865, 34915, 55395, 34919, 51305, 12394, 115, 51319, 49278, 8322, 57476, 12420, 8326, 49287, 26761, 43149, 10386, 63644, 2205, 24737, 49316, 51370, 2222, 59567, 4276, 39099, 47312, 43217, 43218, 10451, 14553, 63705, 63711, 53474, 4322, 6371, 55524, 51436, 28909, 8430, 63725, 39150, 57590, 35068, 33021, 45310, 57599, 22784, 18690, 43272, 24841, 28940, 26894, 59663, 10512, 30992, 57621, 61720, 59673, 14619, 18719, 12578, 49443, 6439, 2351, 6448, 57653, 59701, 53567, 16704, 43330, 47426, 37188, 2373, 55623, 49482, 61772, 53593, 63837, 8549, 55657, 10607, 14706, 55667, 22905, 12668, 8573, 6524, 45441, 386, 57734, 43398, 2454, 57754, 22940, 55716, 31143, 18856, 51625, 20906, 22951, 29100, 49584, 27057, 43444, 10678, 4535, 51641, 12736, 22979, 6596, 6603, 51672, 2520, 49632, 20963, 25069, 495, 33269, 55799, 43513, 29177, 33278, 33284, 57860, 4615, 64007, 59919, 41496, 43548, 41502, 59934, 4640, 37410, 57891, 21033, 33327, 45615, 37430, 62007, 23099, 10824, 62025, 47691, 35406, 59982, 45649, 29271, 19032, 37466, 33378, 21092, 25189, 12902, 47716, 8809, 25201, 62066, 39541, 23158, 27255, 19069, 638, 45694, 53889, 31362, 31365, 43657, 55947, 23180, 10905, 62110, 21156, 55974, 10920, 8873, 43689, 17067, 62125, 33458, 43702, 29372, 60094, 10943, 41667, 51914, 13011, 23251, 33494, 31454, 41695, 27360, 37601, 35557, 37607, 58088, 25322, 13039, 6897, 23282, 51961, 17146, 19199, 51977, 6923, 25357, 23309, 43795, 29469, 17186, 52002, 37666, 43815, 62250, 17195, 19246, 15153, 29491, 6967, 35643, 64315, 6975, 29504, 64321, 45896, 52047, 6996, 64341, 855, 39773, 7005, 47970, 58212, 9060, 23396, 45932, 880, 19313, 50035, 58235, 58240, 52098, 50051, 45954, 64388, 11142, 56198, 58252, 52111, 48017, 916, 48023, 48025, 56217, 58268, 41888, 33703, 56233, 45996, 39852, 5039, 11193, 23481, 19392, 37829, 17351, 15305, 52179, 50134, 11222, 60379, 29659, 50141, 56286, 19426, 7141, 44006, 56296, 11243, 15343, 5109, 19449, 39933, 23553, 56321, 17411, 11271, 58376, 13325, 41998, 11279, 54286, 58387, 21530, 3099, 37916, 23578, 5156, 54313, 9264, 62512, 39987, 54326, 39990, 48183, 27720, 52300, 52322, 1124, 56422, 37993, 17516, 33901, 50285, 21612, 29808, 56430, 1142, 64630, 33920, 35970, 62594, 21635, 42120, 13448, 23690, 56459, 33936, 7314, 52383, 13471, 23713, 54435, 9380, 5286, 33961, 19625, 52403, 33972, 46263, 33976, 9403, 38082, 48326, 17607, 46280, 19657, 56521, 9420, 23756, 48336, 44241, 27858, 23763, 23777, 34018, 25832, 58601, 48363, 23787, 21742, 56563, 7425, 9474, 27906, 29957, 58637, 21774, 60687, 5391, 54548, 11544, 32024, 46370, 23845, 60711, 42280, 38186, 5419, 58670, 13614, 3376, 44339, 15668, 38197, 44350, 50500, 60740, 11589, 3398, 64836, 5450, 38220, 19790, 3412, 46422, 32087, 1372, 48479, 54631, 50537, 17770, 11626, 50540, 60787, 32115, 60790, 7543, 34177, 28033, 60802, 25991, 52615, 17802, 38285, 64915, 7574, 19864, 58777, 7577, 54684, 48542, 58784, 34211, 52643, 23975, 17832, 23979, 17836, 9644, 58797, 56747, 48562, 54713, 46526, 5566, 60864, 54729, 24013, 50643, 1492, 11735, 38360, 54745, 24024, 62939, 42462, 9694, 60894, 5598, 50660, 34277, 1510, 46566, 7657, 21996, 38384, 42481, 28148, 24060, 44542, 5638, 9735, 52743, 34313, 5640, 13834, 65037, 44562, 42520, 5656, 13849, 24088, 34333, 15910, 9769, 28203, 5677, 42544, 38453, 44603, 7740, 11840, 44619, 20044, 46668, 40524, 48719, 40527, 46673, 61015, 50776, 30295, 65118, 63072, 7777, 28269, 26222, 15983, 42624, 48768, 26245, 65170, 11924, 46749, 59044, 48807, 36523, 28335, 9905, 18097, 44729, 48825, 30398, 52927, 24259, 7887, 65231, 1755, 5855, 55011, 36582, 36584, 57072, 22260, 14068, 38644, 28407, 14072, 20222, 44803, 28426, 20238, 12048, 7952, 61205, 14103, 36641, 16161, 20263, 10024, 1835, 55094, 42813, 12094, 5949, 34624, 16193, 30530, 20296, 55112, 20300, 65358, 51024, 28499, 32596, 46936, 48987, 5980, 10077, 61279, 63331, 63335, 44910, 28529, 6023, 6024, 22408, 55179, 28557, 16278, 40854, 10137, 22434, 44963, 47012, 22437, 16294, 44969, 12205, 30649, 12222, 24513, 8130, 1987, 26566, 6091, 49100, 63443, 4054, 63448, 45028, 8170, 53230, 24558, 16377, 20474, 4092, 63485]}\n"
     ]
    }
   ],
   "source": [
    "data_dict={}\n",
    "for i in range(len(cooccurrence_feature)):\n",
    "    data_dict[formatted_sample['image_name'][i]]=cooccurrence_feature[i]\n",
    "print(data_dict)\n",
    "batch_size = 10000\n",
    "for i in range(0, len(data_dict), batch_size):\n",
    "    batch_dict = dict(list(data_dict.items())[i:i+batch_size])\n",
    "    torch.save(batch_dict, f'data_batch_{i // batch_size}.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
