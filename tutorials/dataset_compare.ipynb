{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IoU between datasets: 0.8492\n",
      "Intersection size: 26799\n",
      "Union size: 31557\n"
     ]
    }
   ],
   "source": [
    "#119 206 633 841\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from datasets import load_from_disk, Dataset, Features, ClassLabel as DsImage\n",
    "import datasets\n",
    "import hashlib\n",
    "\n",
    "# 加载两个数据集\n",
    "data1 = load_from_disk(\"/aifs4su/yaodong/changye/data/AA_preference_mistral-7b_cosi_cut/top1-25\")\n",
    "data2 = load_from_disk(\"/aifs4su/yaodong/changye/data/AA_preference_mistral-7b_cosi_pair_cut/top1-25\")\n",
    "data3=load_from_disk(\"/aifs4su/yaodong/changye/data/AA_preference_mistral-7b_cosi_cut/top26-50\")\n",
    "data4=load_from_disk(\"/aifs4su/yaodong/changye/data/AA_preference_mistral-7b_cosi_pair_cut/top26-50\")\n",
    "data5=load_from_disk(\"/aifs4su/yaodong/changye/data/AA_preference_mistral-7b_cosi_cut/top51-75\")\n",
    "data6=load_from_disk(\"/aifs4su/yaodong/changye/data/AA_preference_mistral-7b_cosi_pair_cut/top51-75\")\n",
    "def generate_text_hash(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate a unique identifier for the given text using SHA-256.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text.\n",
    "\n",
    "    Returns:\n",
    "        str: Unique hash for the text.\n",
    "    \"\"\"\n",
    "    hash_object = hashlib.sha256(text.encode('utf-8'))\n",
    "    return hash_object.hexdigest()\n",
    "\n",
    "def extract_hashes(dataset):\n",
    "    \"\"\"\n",
    "    For each item in the dataset, compute its hash based on question + response_1 + response_2.\n",
    "\n",
    "    Args:\n",
    "        dataset: A HuggingFace Dataset object\n",
    "\n",
    "    Returns:\n",
    "        Set of unique hashes.\n",
    "    \"\"\"\n",
    "    hash_set = set()\n",
    "    for item in dataset:\n",
    "        key = item['question'] + item['response_1'] + item['response_2']\n",
    "        hash_set.add(generate_text_hash(key))\n",
    "    return hash_set\n",
    "\n",
    "# 提取两个数据集的哈希集合\n",
    "hashes1 = extract_hashes(data1)\n",
    "hashes2 = extract_hashes(data2)\n",
    "hashes3= extract_hashes(data3)\n",
    "hashes4= extract_hashes(data4)\n",
    "hashes5= extract_hashes(data5)\n",
    "hashes6= extract_hashes(data6)\n",
    "H1=hashes1|hashes3|hashes5\n",
    "H2=hashes2|hashes4|hashes6\n",
    "# 计算交集与并集\n",
    "intersection = H1 & H2\n",
    "union = H1 | H2\n",
    "\n",
    "# 计算IoU\n",
    "iou = len(intersection) / len(union) if union else 0\n",
    "\n",
    "print(f\"IoU between datasets: {iou:.4f}\")\n",
    "print(f\"Intersection size: {len(intersection)}\")\n",
    "print(f\"Union size: {len(union)}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lcy_interp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
