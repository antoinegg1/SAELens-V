{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab # type: ignore\n",
    "    from google.colab import output\n",
    "    COLAB = True\n",
    "    %pip install sae-lens transformer-lens\n",
    "except:\n",
    "    COLAB = False\n",
    "    from IPython import get_ipython # type: ignore\n",
    "    ipython = get_ipython(); assert ipython is not None\n",
    "    ipython.run_line_magic(\"load_ext\", \"autoreload\")\n",
    "    ipython.run_line_magic(\"autoreload\", \"2\")\n",
    "\n",
    "# Standard imports\n",
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import plotly.express as px\n",
    "\n",
    "# Imports for displaying vis in Colab / notebook\n",
    "import webbrowser\n",
    "import http.server\n",
    "import socketserver\n",
    "import threading\n",
    "PORT = 8000\n",
    "\n",
    "torch.set_grad_enabled(False);\n",
    "\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cuda:6\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "def display_vis_inline(filename: str, height: int = 850):\n",
    "    '''\n",
    "    Displays the HTML files in Colab. Uses global `PORT` variable defined in prev cell, so that each\n",
    "    vis has a unique port without having to define a port within the function.\n",
    "    '''\n",
    "    if not(COLAB):\n",
    "        webbrowser.open(filename);\n",
    "\n",
    "    else:\n",
    "        global PORT\n",
    "\n",
    "        def serve(directory):\n",
    "            os.chdir(directory)\n",
    "\n",
    "            # Create a handler for serving files\n",
    "            handler = http.server.SimpleHTTPRequestHandler\n",
    "\n",
    "            # Create a socket server with the handler\n",
    "            with socketserver.TCPServer((\"\", PORT), handler) as httpd:\n",
    "                print(f\"Serving files from {directory} on port {PORT}\")\n",
    "                httpd.serve_forever()\n",
    "\n",
    "        thread = threading.Thread(target=serve, args=(\"/content\",))\n",
    "        thread.start()\n",
    "\n",
    "        output.serve_kernel_port_as_iframe(PORT, path=f\"/{filename}\", height=height, cache_in_notebook=True)\n",
    "\n",
    "        PORT += 1\n",
    "        \n",
    "        \n",
    "from datasets import Dataset, DatasetDict, IterableDataset, load_dataset,load_from_disk\n",
    "from transformer_lens import HookedTransformer\n",
    "from typing import Any, Generator, Iterator, Literal, cast\n",
    "from sae_lens import SAE\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    LlavaNextForConditionalGeneration,\n",
    "    LlavaNextProcessor,\n",
    "    AutoModelForCausalLM,\n",
    ")\n",
    "\n",
    "from transformer_lens.HookedLlava import HookedLlava\n",
    "MODEL_NAME = \"llava-hf/llava-v1.6-mistral-7b-hf\"\n",
    "model_path=\"/home/saev/changye/model/llava\"\n",
    "processor = LlavaNextProcessor.from_pretrained(model_path)\n",
    "vision_model = LlavaNextForConditionalGeneration.from_pretrained(\n",
    "        model_path, \n",
    "        torch_dtype=torch.float32, \n",
    "        low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "vision_tower = vision_model.vision_tower.to(\"cuda:6\")\n",
    "multi_modal_projector = vision_model.multi_modal_projector.to(\"cuda:6\")\n",
    "# 加载 HookedTransformer 语言模型\n",
    "hook_language_model = HookedLlava.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        hf_model=vision_model.language_model,\n",
    "        device=\"cuda:6\", \n",
    "        fold_ln=False,\n",
    "        center_writing_weights=False,\n",
    "        center_unembed=False,\n",
    "        tokenizer=None,\n",
    "        dtype=torch.float32,\n",
    "        vision_tower=vision_tower,\n",
    "        multi_modal_projector=multi_modal_projector,\n",
    "        n_devices=2,\n",
    "    )\n",
    "# del vision_model\n",
    "# the cfg dict is returned alongside the SAE since it may contain useful information for analysing the SAE (eg: instantiating an activation store)\n",
    "# Note that this is not the same as the SAEs config dict, rather it is whatever was in the HF repo, from which we can extract the SAE config dict\n",
    "# We also return the feature sparsities which are stored in HF for convenience. \n",
    "# sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "#     release = \"gpt2-small-res-jb\", # see other options in sae_lens/pretrained_saes.yaml\n",
    "#     sae_id = \"blocks.8.hook_resid_pre\", # won't always be a hook point\n",
    "#     device = device\n",
    "# )\n",
    "\n",
    "sae = SAE.load_from_pretrained(\n",
    "    path = \"/home/saev/changye/checkpoints-V/kxpk98cr/final_122880000\",\n",
    "    device =\"cuda:7\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens.utils import tokenize_and_concatenate\n",
    "import transformer_lens.utils as utils\n",
    "dataset_path=\"/home/saev/changye/data/obelics100k-tokenized-llava4096_w/batch_1\"\n",
    "try:\n",
    "    dataset = (\n",
    "        load_dataset(\n",
    "                dataset_path,\n",
    "                split=\"train\",\n",
    "                streaming=False,\n",
    "                trust_remote_code=False,  # type: ignore\n",
    "        )\n",
    "        if isinstance(dataset_path, str)\n",
    "            else dataset_path\n",
    "        )\n",
    "except Exception as e:\n",
    "    dataset = (\n",
    "        load_from_disk(\n",
    "                dataset_path,\n",
    "            )\n",
    "            if isinstance(dataset_path, str)\n",
    "            else dataset_path\n",
    "        )\n",
    "if isinstance(dataset, (Dataset, DatasetDict)):\n",
    "        dataset = cast(Dataset | DatasetDict, dataset)\n",
    "# dataset_sample = next(iter(dataset))\n",
    "columns_to_read=[\"input_ids\",\"pixel_values\",\"attention_mask\",\"image_sizes\"]\n",
    "ds_context_size = len(dataset[\"input_ids\"])\n",
    "if hasattr(dataset, \"set_format\"):\n",
    "    dataset.set_format(type=\"torch\", columns=columns_to_read)\n",
    "    print(\"dataset set format\")\n",
    "    \n",
    "batch_size = 2\n",
    "batch = dataset[:batch_size]\n",
    "batch_tokens = {\n",
    "    \"input_ids\": batch[\"input_ids\"],\n",
    "    \"pixel_values\": batch[\"pixel_values\"],\n",
    "    \"attention_mask\": batch[\"attention_mask\"],\n",
    "    \"image_sizes\": batch[\"image_sizes\"],\n",
    "}\n",
    "\n",
    "sae.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sae.eval()  # prevents error if we're expecting a dead neuron mask for who grads\n",
    "\n",
    "with torch.no_grad():\n",
    "    # activation store can give us tokens.\n",
    "    _, cache = hook_language_model.run_with_cache(batch_tokens, prepend_bos=True, names_filter=lambda name: name == sae.cfg.hook_name)\n",
    "\n",
    "    # Use the SAE\n",
    "    feature_acts = sae.encode(cache[sae.cfg.hook_name])\n",
    "    sae_out = sae.decode(feature_acts)\n",
    "\n",
    "    # save some room\n",
    "    del cache\n",
    "\n",
    "    # ignore the bos token, get the number of features that activated in each token, averaged accross batch and position\n",
    "    l0 = (feature_acts[:, 1:] > 0).float().sum(-1).detach()\n",
    "    print(\"average l0\", l0.mean().item())\n",
    "    px.histogram(l0.flatten().cpu().numpy()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "changyesae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
