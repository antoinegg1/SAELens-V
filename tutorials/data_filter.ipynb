{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import Dataset, DatasetDict, IterableDataset, load_dataset,load_from_disk\n",
    "import hashlib\n",
    "import os\n",
    "import tqdm\n",
    "# 示例文件路径\n",
    "file_path = \"/data/changye/data/Align-Anything/Align-Anything_cooccur.pt\"\n",
    "\n",
    "os.environ['HF_DATASETS_CACHE']='/data/changye/tmp'\n",
    "# 读取 .pt 文件\n",
    "data = torch.load(file_path)\n",
    "\n",
    "dataset_path=\"/data/changye/data/Align-Anything-TI2T-Instruction-100K\"\n",
    "train_dataset = load_dataset(\n",
    "            dataset_path,\n",
    "            split=\"train\",\n",
    "            trust_remote_code=True,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_list=[]\n",
    "for data_l in data:\n",
    "    keys=list(data_l.keys())\n",
    "    for key in keys:\n",
    "        data_list.append((key,data_l[key]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_hash(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate a unique identifier for the given text using SHA-256.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text.\n",
    "\n",
    "    Returns:\n",
    "        str: Unique hash for the text.\n",
    "    \"\"\"\n",
    "    hash_object = hashlib.sha256(text.encode('utf-8'))\n",
    "    return hash_object.hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = {}\n",
    "for j, item in tqdm.tqdm(enumerate(train_dataset), desc=\"Processing data_list\", unit=\"item\"):\n",
    "    key = generate_text_hash(item['prompt']+item['response'])\n",
    "    if key not in dataset_dict:\n",
    "        dataset_dict[key] = []\n",
    "    dataset_dict[key].append(j)  # 记录每个idx对应的所有匹配位置\n",
    "\n",
    "# 2. 生成 formatted_dataset，确保按照顺序匹配\n",
    "formatted_dataset = []\n",
    "index_set = set()  # 用来记录已经匹配过的 train_dataset 索引\n",
    "\n",
    "# 使用 tqdm 显示进度条\n",
    "for i in tqdm.tqdm(range(len(data_list)), desc=\"Processing data_list\", unit=\"item\"):\n",
    "    key_datalist, value_datalist = data_list[i]\n",
    "    \n",
    "    # 查找是否存在匹配的key\n",
    "    if key_datalist in dataset_dict:\n",
    "        # 遍历匹配的索引\n",
    "        for j in dataset_dict[key_datalist]:\n",
    "            if j not in index_set:  # 如果该项未匹配过\n",
    "                # 如果匹配，则将 value 从 data_list 加到 train_dataset 项中\n",
    "                new_item = train_dataset[j].copy()  # 拷贝 train_dataset 项\n",
    "                new_item[\"Coocurr\"] = value_datalist\n",
    "                formatted_dataset.append(new_item)\n",
    "                \n",
    "                # 将已匹配的索引添加到 index_set\n",
    "                index_set.add(j)\n",
    "                break  # 找到一个匹配就跳出内层循环，继续下一个 data_list 的元素\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(formatted_dataset))\n",
    "print(formatted_dataset[-1])\n",
    "# print(len(dataset_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义文件路径\n",
    "cosi_file_path = \"/data/changye/data/Align-Anything/Align-Anything_cosi_weight/cosi_feature_list.txt\"  # 将此替换为你的文件路径\n",
    "\n",
    "# 读取文件并转换为字典\n",
    "osi_dict = {}\n",
    "with open(cosi_file_path, \"r\") as file:\n",
    "    for line in file:\n",
    "        key, value = line.strip().split(\",\")  # 按逗号分割每行\n",
    "        osi_dict[int(key)] = float(value)    # 将 key 转为 int, value 转为 float\n",
    "\n",
    "# 输出结果\n",
    "print(osi_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_cosi_key=set(osi_dict.keys())\n",
    "cosi_coocur_data={}\n",
    "for f_data in tqdm.tqdm(formatted_dataset):\n",
    "    score=0\n",
    "    for value in f_data['Coocurr']:\n",
    "        if value in set_cosi_key:\n",
    "            score+=osi_dict[value]\n",
    "    f_data[\"Cooccur_score\"]=score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(formatted_dataset[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bf886e4f2884aaf85022b0628846b0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/13 shards):   0%|          | 0/99160 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = Dataset.from_list(formatted_dataset)\n",
    "dataset.save_to_disk(\"/data/changye/data/Align-Anything-cosi-full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 根据 l0 字段从大到小对数据集排序\n",
    "formatted_dataset_sorted = sorted(formatted_dataset, key=lambda x: x[\"Cooccur_score\"], reverse=True)\n",
    "\n",
    "# 2. 将数据集分成四个分位\n",
    "num_samples = len(formatted_dataset_sorted)\n",
    "q1 = int(num_samples * 0.25)\n",
    "q2 = int(num_samples * 0.5)\n",
    "q3 = int(num_samples * 0.75)\n",
    "\n",
    "# 将数据集切分为四个分位\n",
    "split_datasets = {\n",
    "    \"q0_25\": formatted_dataset_sorted[:q1],\n",
    "    \"q25_50\": formatted_dataset_sorted[q1:q2],\n",
    "    \"q50_75\": formatted_dataset_sorted[q2:q3],\n",
    "    \"q75_100\": formatted_dataset_sorted[q3:]\n",
    "}\n",
    "\n",
    "# 3. 转换为 HuggingFace Dataset 格式并保存，显示进度条\n",
    "hf_datasets = {}\n",
    "for split_name, split_data in tqdm.tqdm(split_datasets.items(), desc=\"Processing splits\", unit=\"split\"):\n",
    "    hf_datasets[split_name] = Dataset.from_dict({k: [d[k] for d in split_data] for k in split_data[0].keys()})\n",
    "    \n",
    "    # 保存每个分位数据集到磁盘\n",
    "    hf_datasets[split_name].save_to_disk(f\"./{split_name}_dataset\")\n",
    "\n",
    "# 输出一下分割的数据集\n",
    "for split_name, dataset in hf_datasets.items():\n",
    "    print(f\"{split_name} dataset:\")\n",
    "    print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(formatted_dataset_sorted[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(formatted_dataset_sorted[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cosi_coocur_data['15557.jpg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#我现在有一个dict，键为'image_name'，值为一个浮点数，我希望按照值的大小进行排序，然后我有一个数据集以'image_name'为索引，结构为Dataset({\n",
    "#     features: ['image', 'question', 'chosen', 'rejected', 'image_name'],\n",
    "#     num_rows: 93258\n",
    "# })\n",
    "#我希望能够按照这个排序对这个dataset进行筛选保存，能够保存前25%，25%到50%这样按百分比保存的值，以huggingfacedataset的形式存储，请补全以下代码\n",
    "# Step 1: Sort image_names based on their corresponding values in 'data'\n",
    "sorted_image_names = sorted(cosi_coocur_data, key=cosi_coocur_data.get, reverse=True)\n",
    "\n",
    "# Step 2: Compute indices for the percentiles\n",
    "total_images = len(sorted_image_names)\n",
    "percentiles = [0.25, 0.5, 0.75, 1.0]\n",
    "indices = [int(total_images * p) for p in percentiles]\n",
    "\n",
    "# Step 3: Create ranges for each percentile\n",
    "start_indices = [0] + indices[:-1]\n",
    "end_indices = indices\n",
    "ranges = list(zip(start_indices, end_indices))\n",
    "\n",
    "# Step 4: Filter and save datasets for each percentile range\n",
    "for idx, (start, end) in enumerate(ranges):\n",
    "    # Get the image_names for the current percentile range\n",
    "    image_name_set = set(sorted_image_names[start:end])\n",
    "\n",
    "    # Filter the dataset based on the image_name_set\n",
    "    percentile_dataset = train_dataset.filter(lambda example: example['image_name'] in image_name_set)\n",
    "\n",
    "    # Save the filtered dataset\n",
    "    percentile_label = f\"{int(percentiles[idx]*100)}%\"\n",
    "    percentile_dataset.save_to_disk(f'percentile_{percentile_label}_dataset')\n",
    "\n",
    "    print(f\"Saved {percentile_label} dataset with {len(percentile_dataset)} samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(cosi_coocur_data.values())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.interpolate import make_interp_spline\n",
    "from collections import Counter\n",
    "\n",
    "# 假设 data_list 是一个包含数值的列表\n",
    "# 例如：\n",
    "# data_list = [1, 2, 2, 3, 3, 3, 4, 4, 4, 4]\n",
    "# values_list = [tensor['Cooccur'].cpu().item() if tensor['Cooccur'].is_cuda else tensor['Cooccur'].item() for tensor in formatted_dataset_sorted]\n",
    "values_list=[tensor[\"Cooccur_score\"] for tensor in formatted_dataset_sorted]\n",
    "# 统计频率\n",
    "num_bins = 10\n",
    "\n",
    "# 生成直方图\n",
    "frequencies, bin_edges = np.histogram(values_list, bins=num_bins)\n",
    "\n",
    "# 计算每个区间的中心点（用于插值）\n",
    "bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "\n",
    "# 使用样条插值将直方图转换为平滑曲线\n",
    "bin_centers_smooth = np.linspace(bin_centers[0], bin_centers[-1], 300)  # 插值点\n",
    "frequencies_smooth = make_interp_spline(bin_centers, frequencies)(bin_centers_smooth)\n",
    "\n",
    "# 绘图\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(bin_centers_smooth, frequencies_smooth, color='orange', lw=2)\n",
    "\n",
    "\n",
    "# 样式设置\n",
    "plt.grid(which='both', linestyle='--', linewidth=0.5, alpha=0.7)\n",
    "plt.xlabel('Cooccuring score of data ', fontsize=14)\n",
    "plt.ylabel('Frequency', fontsize=14)\n",
    "plt.title('The distribution of Align-Anything data based on cooccuring score', fontsize=16)\n",
    "plt.legend(fontsize=12)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
