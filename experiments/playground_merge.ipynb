{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  # for google colab users\n",
    "    import google.colab # type: ignore\n",
    "    from google.colab import output\n",
    "    COLAB = True\n",
    "    %pip install sae-lens transformer-lens\n",
    "except:\n",
    "  # for local setup\n",
    "    COLAB = False\n",
    "    from IPython import get_ipython # type: ignore\n",
    "    ipython = get_ipython(); assert ipython is not None\n",
    "    ipython.run_line_magic(\"load_ext\", \"autoreload\")\n",
    "    ipython.run_line_magic(\"autoreload\", \"2\")\n",
    "\n",
    "# Imports for displaying vis in Colab / notebook\n",
    "import webbrowser\n",
    "import http.server\n",
    "import socketserver\n",
    "import threading\n",
    "PORT = 8000\n",
    "\n",
    "# general imports\n",
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import plotly.express as px\n",
    "\n",
    "torch.set_grad_enabled(False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_vis_inline(filename: str, height: int = 850):\n",
    "    '''\n",
    "    Displays the HTML files in Colab. Uses global `PORT` variable defined in prev cell, so that each\n",
    "    vis has a unique port without having to define a port within the function.\n",
    "    '''\n",
    "    if not(COLAB):\n",
    "        webbrowser.open(filename);\n",
    "\n",
    "    else:\n",
    "        global PORT\n",
    "\n",
    "        def serve(directory):\n",
    "            os.chdir(directory)\n",
    "\n",
    "            # Create a handler for serving files\n",
    "            handler = http.server.SimpleHTTPRequestHandler\n",
    "\n",
    "            # Create a socket server with the handler\n",
    "            with socketserver.TCPServer((\"\", PORT), handler) as httpd:\n",
    "                print(f\"Serving files from {directory} on port {PORT}\")\n",
    "                httpd.serve_forever()\n",
    "\n",
    "        thread = threading.Thread(target=serve, args=(\"/content\",))\n",
    "        thread.start()\n",
    "\n",
    "        output.serve_kernel_port_as_iframe(PORT, path=f\"/{filename}\", height=height, cache_in_notebook=True)\n",
    "\n",
    "        PORT += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/aifs4su/yaodong/miniconda3/envs/hantao_gpu/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# package import\n",
    "from torch import Tensor\n",
    "from transformer_lens import utils\n",
    "from functools import partial\n",
    "from jaxtyping import Int, Float\n",
    "\n",
    "device = ['cuda:1', 'cuda:2']\n",
    "# device setup\n",
    "# if torch.backends.mps.is_available():\n",
    "#     device = \"mps\"\n",
    "# else:\n",
    "#     device = \"cuda:2\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.33it/s]\n",
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gemma-2b-it into HookedTransformer\n",
      "blocks.12.hook_resid_post\n"
     ]
    }
   ],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "from sae_lens import SAE\n",
    "from sae_lens.toolkit.pretrained_saes import get_gpt2_res_jb_saes\n",
    "\n",
    "# Choose a layer you want to focus on\n",
    "# For this tutorial, we're going to use layer 2\n",
    "layer = 12\n",
    "\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "    \n",
    "local_model_0 = AutoModelForCausalLM.from_pretrained(\"/aifs4su/yaodong/models/gemma/gemma-2b-it\")\n",
    "\n",
    "# get model\n",
    "model_0 = HookedTransformer.from_pretrained(\"gemma-2b-it\", hf_model = local_model_0, device = device[0])\n",
    "\n",
    "# get the SAE for this layer\n",
    "sae_0, cfg_dict_0, _ = SAE.from_pretrained(\n",
    "    release = \"gemma-2b-it-res-jb\",\n",
    "    sae_id = f\"blocks.{layer}.hook_resid_post\",\n",
    "    device = device[0]\n",
    ")\n",
    "\n",
    "# get hook point\n",
    "hook_point_0 = sae_0.cfg.hook_name\n",
    "print(hook_point_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.30it/s]\n",
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gemma-2b-it into HookedTransformer\n",
      "blocks.12.hook_resid_post\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "    \n",
    "local_model_1 = AutoModelForCausalLM.from_pretrained(\"/aifs4su/yaodong/projects/hantao/personal/models/output_gemma_it_sft0716\")\n",
    "\n",
    "# get model\n",
    "model_1 = HookedTransformer.from_pretrained(\"gemma-2b-it\", hf_model = local_model_1, device = device[1])\n",
    "\n",
    "# get the SAE for this layer\n",
    "sae_1, cfg_dict_1, _ = SAE.from_pretrained(\n",
    "    release = \"gemma-2b-it-res-jb\",\n",
    "    sae_id = f\"blocks.{layer}.hook_resid_post\",\n",
    "    device = device[1]\n",
    ")\n",
    "\n",
    "# get hook point\n",
    "hook_point_1 = sae_1.cfg.hook_name\n",
    "print(hook_point_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv_prompt = \" <start_of_turn>user\\n Fuck you you asshole! \\n<start_of_turn>model\\n Don't say that.\"\n",
    "# sv_prompt = \"Fuck you, you asshole!\"\n",
    "# sv_prompt = \"Where is the golden bridge?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "torch.return_types.topk(\n",
      "values=tensor([[[35.2354, 35.2019, 34.1813, 34.1702, 33.1085],\n",
      "         [ 5.2214,  1.9703,  1.9544,  1.0701,  1.0042],\n",
      "         [ 4.7250,  2.1992,  1.8401,  1.6175,  1.3847],\n",
      "         [ 6.5303,  2.2195,  2.1185,  1.0758,  1.0517],\n",
      "         [ 4.3880,  2.2728,  0.9632,  0.7951,  0.6092],\n",
      "         [ 8.7513,  5.4801,  4.0706,  2.1339,  2.0712],\n",
      "         [ 4.9153,  4.3126,  3.7675,  2.2982,  1.7447],\n",
      "         [ 4.8043,  3.1073,  2.0043,  1.9802,  1.7382],\n",
      "         [ 5.2204,  3.4372,  2.7366,  2.4824,  2.3386],\n",
      "         [ 5.8967,  1.8846,  1.5570,  1.5153,  1.5003],\n",
      "         [ 5.2550,  2.5362,  2.0057,  1.8109,  1.7355],\n",
      "         [ 2.6782,  2.3030,  2.2487,  1.5172,  1.1844],\n",
      "         [ 4.5712,  2.4375,  2.2674,  1.5174,  1.3008],\n",
      "         [ 6.0807,  4.5854,  3.9937,  2.5786,  1.9437],\n",
      "         [ 5.6117,  3.5105,  2.0435,  1.6620,  1.4337],\n",
      "         [ 7.3446,  5.2239,  1.4832,  1.2312,  1.2179],\n",
      "         [ 3.3165,  2.2162,  1.9631,  1.4558,  1.2058],\n",
      "         [ 7.3483,  5.3328,  2.2691,  2.2462,  1.4896],\n",
      "         [10.4024,  3.9256,  2.7927,  2.5342,  2.0089],\n",
      "         [ 3.6023,  3.4124,  3.2766,  2.8364,  2.6653],\n",
      "         [ 4.5093,  2.7117,  2.3296,  2.2448,  2.0206]]], device='cuda:1'),\n",
      "indices=tensor([[[11609, 15572, 13161,  7063,  1111],\n",
      "         [10639, 13891,  6948, 14960,  6060],\n",
      "         [13602,  7628,  7731,  6948, 13287],\n",
      "         [12250, 15945,  6948, 14162, 13122],\n",
      "         [15945, 14162,  8185,  2230,  6948],\n",
      "         [15936, 13237,  8706, 15164,  6619],\n",
      "         [15936,  2382,  7823,   573,  8706],\n",
      "         [14700, 13918,   573,  7823, 11815],\n",
      "         [13237, 12083,   221, 13918, 13163],\n",
      "         [ 8860, 14018,  8757, 16313,   573],\n",
      "         [10639, 14018,  8860,  8757, 11915],\n",
      "         [14151, 14018,  9414,  8757, 10639],\n",
      "         [13602,  7731,  7628, 13287,  6948],\n",
      "         [14151,   616, 14018, 14162,  8757],\n",
      "         [14151, 14018, 14162,  8757,  4597],\n",
      "         [15320, 14151,  2542,    85,   861],\n",
      "         [ 8185,  8772, 14151,  7324, 14018],\n",
      "         [   85, 14151,  8757, 12762, 11270],\n",
      "         [ 9120, 14151, 11270, 14767, 11548],\n",
      "         [14151,  9120,  4183, 11734, 14767],\n",
      "         [14151,  2542, 12762,  7808,   202]]], device='cuda:1'))\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "sv_logits_0, cache_0 = model_0.run_with_cache(sv_prompt, prepend_bos=True)\n",
    "tokens_0 = model_0.to_tokens(sv_prompt)\n",
    "print(tokens_0.shape[1])\n",
    "\n",
    "# get the feature activations from our SAE\n",
    "sv_feature_acts_0 = sae_0.encode(cache_0[hook_point_0])\n",
    "\n",
    "# get sae_out\n",
    "sae_out_0 = sae_0.decode(sv_feature_acts_0)\n",
    "\n",
    "# print out the top activations, focus on the indices\n",
    "topk1 = torch.topk(sv_feature_acts_0, 5)\n",
    "print(torch.topk(sv_feature_acts_0, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[     2, 235248,    106,   1645,    108,  48075,    692,    692,  73663,\n",
      "         235341, 235248,    108,    106,   2516,    108,   4257, 235303, 235251,\n",
      "           1931,    674, 235265]], device='cuda:2')\n",
      "torch.return_types.topk(\n",
      "values=tensor([[[30.0818, 30.0677, 29.2058, 29.1572, 28.3047],\n",
      "         [ 4.2067,  2.3358,  1.5934,  1.3462,  1.0896],\n",
      "         [ 4.2978,  1.8611,  1.5436,  1.2794,  0.9339],\n",
      "         [ 7.0984,  1.8055,  1.4783,  1.4376,  0.9996],\n",
      "         [ 3.6766,  0.9447,  0.9242,  0.6379,  0.5407],\n",
      "         [ 8.5701,  6.5818,  4.1991,  1.8930,  1.7804],\n",
      "         [ 5.6836,  3.2678,  3.1601,  1.9078,  1.5844],\n",
      "         [ 5.9737,  4.3934,  2.0849,  1.9427,  1.8765],\n",
      "         [ 5.8809,  3.6808,  3.2547,  3.0860,  2.0637],\n",
      "         [ 7.4964,  2.1445,  1.5424,  1.4714,  1.4118],\n",
      "         [ 5.4907,  2.7920,  2.2104,  1.7469,  1.2382],\n",
      "         [ 2.9947,  1.6853,  1.4526,  1.1380,  1.1139],\n",
      "         [ 3.9846,  2.1403,  1.5395,  1.1767,  0.9074],\n",
      "         [ 9.0793,  2.2083,  1.8096,  1.2728,  1.2684],\n",
      "         [ 2.7215,  2.2284,  1.9242,  1.3196,  1.2692],\n",
      "         [ 9.2731,  2.5837,  1.3435,  1.1867,  1.0274],\n",
      "         [ 3.3840,  2.3177,  1.4449,  0.9367,  0.9040],\n",
      "         [10.5003,  2.1489,  2.0319,  1.9906,  1.6803],\n",
      "         [11.0785,  3.0881,  2.7962,  2.0068,  1.9345],\n",
      "         [ 3.3853,  3.3354,  3.2847,  2.0839,  1.6864],\n",
      "         [ 1.9261,  1.7813,  1.5280,  1.3548,  1.2354]]], device='cuda:2'),\n",
      "indices=tensor([[[11609, 15572, 13161,  7063,  1111],\n",
      "         [10639, 13891, 14960,  6948,  6060],\n",
      "         [13602,  7628,  7731,  6948, 13287],\n",
      "         [12250,  6948, 15945, 13122,  9552],\n",
      "         [15945, 14162,  2230, 14960,  1633],\n",
      "         [15936, 13237,  8706, 16028,  6619],\n",
      "         [ 2382,  7823, 15936,  5952,  5007],\n",
      "         [14700, 13918, 11815,  9678,  1707],\n",
      "         [13237,   221, 12083,   164, 11776],\n",
      "         [ 8860, 16313,  8757,  6164,  8394],\n",
      "         [10639,  8860, 11915,  8757,  1145],\n",
      "         [ 9414, 10639,  6164,   573,  8757],\n",
      "         [13602,  7731,  7628,  1796, 13287],\n",
      "         [  616, 14151,   202,  1056,  1050],\n",
      "         [14891,   198, 14151,   202, 15011],\n",
      "         [15320, 14151,  5867,   202,  9632],\n",
      "         [ 8772,  8185,  7324,  5450, 13279],\n",
      "         [   85, 14151,  1541,  5867, 11270],\n",
      "         [ 9120, 11270,    85, 14151,  5867],\n",
      "         [ 4183, 11734,  9120, 11270, 14151],\n",
      "         [ 7808,  5515, 14907, 14151, 13505]]], device='cuda:2'))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sv_logits_1, cache_1 = model_1.run_with_cache(sv_prompt, prepend_bos=True)\n",
    "tokens_1 = model_1.to_tokens(sv_prompt)\n",
    "print(tokens_1)\n",
    "\n",
    "# get the feature activations from our SAE\n",
    "sv_feature_acts_1 = sae_1.encode(cache_1[hook_point_1])\n",
    "\n",
    "# get sae_out\n",
    "sae_out_1 = sae_1.decode(sv_feature_acts_1)\n",
    "\n",
    "# print out the top activations, focus on the indices\n",
    "topk2 = torch.topk(sv_feature_acts_1, 5)\n",
    "print(torch.topk(sv_feature_acts_1, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intersection Counts: tensor([2, 3, 3, 3])\n",
      "Average Value Changes: tensor([0.1870, 0.0505, 0.2290, 0.0445])\n"
     ]
    }
   ],
   "source": [
    "topk1_values_cpu = topk1.values.to('cpu')\n",
    "topk1_indices_cpu = topk1.indices.to('cpu')\n",
    "topk2_values_cpu = topk2.values.to('cpu')\n",
    "topk2_indices_cpu = topk2.indices.to('cpu')\n",
    "\n",
    "# Create new structures on the CPU\n",
    "# topk1_cpu = torch.return_types.topk(values=topk1_values_cpu, indices=topk1_indices_cpu)\n",
    "# topk2_cpu = torch.return_types.topk(values=topk2_values_cpu, indices=topk2_indices_cpu)\n",
    "\n",
    "intersection_counts = []\n",
    "value_changes = []\n",
    "\n",
    "# for col in range(topk1_indices_cpu.size(1)):  # Assuming same number of columns\n",
    "for col in range(15, 19):\n",
    "    indices1 = topk1_indices_cpu[:,col,:]\n",
    "    indices2 = topk2_indices_cpu[:,col,:]\n",
    "    values1 = topk1_values_cpu[:,col,:]\n",
    "    values2 = topk2_values_cpu[:,col,:]\n",
    "\n",
    "    # Compute intersection manually\n",
    "    mask = (indices1.unsqueeze(-1) == indices2.unsqueeze(-2)).any(-1)\n",
    "    common_indices1 = indices1[mask]\n",
    "    common_indices2 = indices2[mask]\n",
    "\n",
    "    intersection_count = common_indices1.size(0)\n",
    "    intersection_counts.append(intersection_count)\n",
    "\n",
    "    # Compute value changes for intersected indices\n",
    "    if intersection_count > 0:\n",
    "        # Matching indices in both tensors\n",
    "        matching_values1 = values1[mask]\n",
    "        matching_values2 = values2[mask]\n",
    "\n",
    "        # Calculate mean of absolute differences\n",
    "        value_change = torch.abs(matching_values1 - matching_values2).mean() /  torch.abs(matching_values1 + matching_values2).mean()\n",
    "    else:\n",
    "        value_change = torch.tensor(0.0)  # If no intersection, set change to 0\n",
    "    value_changes.append(value_change)\n",
    "\n",
    "# Output results\n",
    "intersection_counts = torch.tensor(intersection_counts)\n",
    "value_changes = torch.tensor(value_changes)\n",
    "\n",
    "print(\"Intersection Counts:\", intersection_counts)\n",
    "print(\"Average Value Changes:\", value_changes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hantao_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
